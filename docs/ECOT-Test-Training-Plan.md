# ECoT éšå¼æ¨ç†ç«¯åˆ°ç«¯æµ‹è¯•è®­ç»ƒè„šæœ¬è®¾è®¡æ–¹æ¡ˆ

## ä¸€ã€æµ‹è¯•ç›®æ ‡

### æ ¸å¿ƒéªŒè¯ç‚¹
1. âœ… **æ•°æ®æµéªŒè¯**: ECOT RLDSæ•°æ® â†’ DataLoader â†’ Batchæ ¼å¼æ­£ç¡®
2. âœ… **TokenizationéªŒè¯**: `@` åˆ†ç•Œç¬¦ + thinking tokensæ­£ç¡®æ’å…¥å’Œtokenize
3. âœ… **å¯¹é½éªŒè¯**: Thinking tokenä½ç½®åœ¨batchä¸­æ­£ç¡®å¯¹é½
4. âœ… **Labelæ©ç éªŒè¯**: Instructionå’Œlatent spanæ­£ç¡®mask
5. âœ… **ForwardéªŒè¯**: 
   - Stage 0: æ™®é€šforward + VLM lossè®¡ç®—
   - Stage 2+: KV-Cacheè¿­ä»£forward + thinking tokenæ›´æ–°
6. âœ… **Lossè®¡ç®—éªŒè¯**: Action loss + VLM lossæ­£ç¡®è®¡ç®—å’Œåˆå¹¶
7. âœ… **æ¢¯åº¦æµéªŒè¯**: åå‘ä¼ æ’­æ­£å¸¸ï¼Œæ¢¯åº¦ä¸ä¸ºNaN/Inf
8. âœ… **å¤šæ­¥è®­ç»ƒéªŒè¯**: è‡³å°‘è¿è¡Œ10-20æ­¥ï¼ŒéªŒè¯ç¨³å®šæ€§

### éç›®æ ‡
- âŒ ä¸è¿½æ±‚è®­ç»ƒæ”¶æ•›ï¼ˆåªéªŒè¯æµç¨‹ï¼‰
- âŒ ä¸è¿›è¡Œå®Œæ•´çš„evaluation
- âŒ ä¸ä¿å­˜checkpointï¼ˆå¯é€‰ï¼‰
- âŒ ä¸ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå•GPUæµ‹è¯•ï¼‰

---

## äºŒã€è„šæœ¬è®¾è®¡æ¶æ„

### 2.1 æ–‡ä»¶ç»“æ„
```
starVLA/
â”œâ”€â”€ test_ecot_training.py          # ä¸»æµ‹è¯•è„šæœ¬ï¼ˆæ–°å»ºï¼‰
â”œâ”€â”€ config/
â”‚   â””â”€â”€ test_ecot_stage0.yaml      # Stage 0æµ‹è¯•é…ç½®ï¼ˆæ–°å»ºï¼‰
â”‚   â””â”€â”€ test_ecot_stage2.yaml      # Stage 2æµ‹è¯•é…ç½®ï¼ˆæ–°å»ºï¼‰
â””â”€â”€ docs/
    â””â”€â”€ ECOT-Test-Training-Plan.md # æœ¬æ–‡æ¡£
```

### 2.2 è„šæœ¬æ¨¡å—åˆ’åˆ†

```python
# test_ecot_training.py ç»“æ„

# 1. å¯¼å…¥æ¨¡å—
# 2. é…ç½®åŠ è½½ä¸éªŒè¯
# 3. æ•°æ®å‡†å¤‡
# 4. æ¨¡å‹æ„å»º
# 5. å‰å‘æµ‹è¯•ï¼ˆæ— æ¢¯åº¦ï¼‰
# 6. è®­ç»ƒå¾ªç¯æµ‹è¯•ï¼ˆæœ‰æ¢¯åº¦ï¼‰
# 7. ç»“æœéªŒè¯ä¸æŠ¥å‘Š
```

---

## ä¸‰ã€è¯¦ç»†è®¾è®¡

### 3.1 é…ç½®æ–‡ä»¶è®¾è®¡

#### Stage 0 é…ç½® (`config/test_ecot_stage0.yaml`)

```yaml
# åŸºç¡€é…ç½®
run_id: "test_ecot_stage0"
run_root_dir: "./test_outputs"
seed: 42
is_debug: false

# æ•°æ®é…ç½®
datasets:
  vla_data:
    dataset_py: "ecot_rlds"
    per_device_batch_size: 2  # ä¿å®ˆè®¾ç½®
    num_workers: 0  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
    image_size: [224, 224]
    ecot:
      data_root_dir: "/share/project/emllm_mnt.1d/mnt/sfs/baaiei/jyShi/rt_newData"
      data_mix: "bridge"
      scheduled_stage: 0  # Stage 0: æ— thinking tokens
      action_dim: 7
      future_action_window_size: 15
      past_action_window_size: 0
      shuffle_buffer_size: 100  # å°bufferï¼Œå¿«é€Ÿæµ‹è¯•
      image_aug: false
      reasoning_json: "/share/project/lvjing/datas/embodied_features_bridge.json"
      load_proprio: true
      lower_case_instruction: true
      train: true

# æ¨¡å‹é…ç½®
framework:
  name: "QwenGR00T"
  enable_latent_reasoning: true  # å¼€å¯ï¼ˆä½†stage 0æ— thinking tokensï¼‰
  latent_reasoning:
    compute_language_loss: true  # æµ‹è¯•VLM lossè®¡ç®—
    vlm_loss_weight: 0.1
    thinking_token: "<|thinking|>"
    start_of_thinking_token: "<|start_of_thinking|>"
    end_of_thinking_token: "<|end_of_thinking|>"
  qwenvl:
    base_vlm: "Qwen/Qwen3-VL-2B-Instruct"
    attn_implementation: "sdpa"
    cache_dir: "./qwen_cache"
    model_max_length: 2048  # å‡å°ä»¥åŠ å¿«æµ‹è¯•
  action_model:
    action_dim: 7
    future_action_window_size: 15
    past_action_window_size: 0

# è®­ç»ƒé…ç½®ï¼ˆæœ€å°åŒ–ï¼‰
trainer:
  max_train_steps: 10  # åªè·‘10æ­¥
  gradient_accumulation_steps: 1
  learning_rate:
    base: 1.0e-5
  optimizer:
    betas: [0.9, 0.95]
    weight_decay: 0.01
    eps: 1.0e-8
  lr_scheduler_type: "constant"
  num_warmup_steps: 0
  gradient_clipping: 1.0
  logging_frequency: 1  # æ¯æ­¥éƒ½log
  save_interval: 1000  # ä¸ä¿å­˜
  eval_interval: 1000  # ä¸eval
  repeated_diffusion_steps: 2  # å‡å°‘diffusion stepsåŠ å¿«æµ‹è¯•

# W&Bé…ç½®ï¼ˆå¯é€‰ï¼‰
wandb_project: "test_ecot"
wandb_entity: null
```

#### Stage 2 é…ç½® (`config/test_ecot_stage2.yaml`)

```yaml
# ä¸Stage 0å¤§éƒ¨åˆ†ç›¸åŒï¼Œå…³é”®å·®å¼‚ï¼š
datasets:
  vla_data:
    ecot:
      scheduled_stage: 2  # Stage 2: æœ‰thinking tokens
      thinking_token_count: 2
      tag2think_count:
        TASK: 1
        PLAN: 1
        "VISIBLE OBJECTS": 1
        "SUBTASK REASONING": 1
        SUBTASK: 1
        "MOVE REASONING": 1
        MOVE: 1
        "GRIPPER POSITION": 1

# å…¶ä»–é…ç½®ç›¸åŒ
```

---

### 3.2 ä¸»æµ‹è¯•è„šæœ¬è®¾è®¡ (`test_ecot_training.py`)

#### æ¨¡å—1: å¯¼å…¥ä¸åˆå§‹åŒ–

```python
"""
ECoT Implicit Reasoning End-to-End Test Training Script

Purpose: Validate the entire pipeline from data loading to training
- Stage 0: No thinking tokens, @ delimiter only
- Stage 2+: With thinking tokens, KV-Cache iterative forward

Usage:
    python test_ecot_training.py --config_yaml config/test_ecot_stage0.yaml
    python test_ecot_training.py --config_yaml config/test_ecot_stage2.yaml
"""

import argparse
import os
import sys
from pathlib import Path
import torch
import numpy as np
from omegaconf import OmegaConf
from tqdm import tqdm

# Local imports
from starVLA.model.framework import build_framework
from starVLA.dataloader import build_dataloader

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["HF_HOME"] = "./qwen_cache"
```

#### æ¨¡å—2: é…ç½®éªŒè¯

```python
def validate_config(cfg):
    """
    éªŒè¯é…ç½®æ–‡ä»¶çš„å®Œæ•´æ€§å’Œåˆç†æ€§
    
    æ£€æŸ¥é¡¹:
    1. å¿…éœ€å­—æ®µå­˜åœ¨
    2. è·¯å¾„æœ‰æ•ˆ
    3. scheduled_stageä¸enable_latent_reasoningä¸€è‡´
    4. batch_sizeåˆç†ï¼ˆå»ºè®®<=4ï¼‰
    """
    print("\n" + "="*80)
    print("ğŸ“‹ Configuration Validation")
    print("="*80)
    
    # æ£€æŸ¥å¿…éœ€å­—æ®µ
    required_fields = [
        "datasets.vla_data.dataset_py",
        "datasets.vla_data.ecot.data_root_dir",
        "datasets.vla_data.ecot.scheduled_stage",
        "framework.name",
        "framework.enable_latent_reasoning",
    ]
    
    # æ£€æŸ¥æ•°æ®è·¯å¾„
    data_root = cfg.datasets.vla_data.ecot.data_root_dir
    reasoning_json = cfg.datasets.vla_data.ecot.reasoning_json
    
    # æ£€æŸ¥batch size
    batch_size = cfg.datasets.vla_data.per_device_batch_size
    
    # æ‰“å°å…³é”®é…ç½®
    print(f"âœ… Stage: {cfg.datasets.vla_data.ecot.scheduled_stage}")
    print(f"âœ… Enable Latent Reasoning: {cfg.framework.enable_latent_reasoning}")
    print(f"âœ… Compute Language Loss: {cfg.framework.latent_reasoning.compute_language_loss}")
    print(f"âœ… Batch Size: {batch_size}")
    print(f"âœ… Max Steps: {cfg.trainer.max_train_steps}")
    
    return True
```

#### æ¨¡å—3: æ•°æ®åŠ è½½æµ‹è¯•

```python
def test_dataloader(cfg):
    """
    æµ‹è¯•æ•°æ®åŠ è½½å’Œæ ¼å¼
    
    éªŒè¯:
    1. DataLoaderå¯ä»¥æ­£å¸¸åˆ›å»º
    2. å¯ä»¥è·å–batch
    3. Batchæ ¼å¼æ­£ç¡®ï¼ˆåŒ…å«å¿…éœ€å­—æ®µï¼‰
    4. æ•°æ®ç±»å‹å’Œshapeæ­£ç¡®
    """
    print("\n" + "="*80)
    print("ğŸ“¦ Testing DataLoader")
    print("="*80)
    
    # åˆ›å»ºdataloader
    dataloader = build_dataloader(cfg=cfg, dataset_py=cfg.datasets.vla_data.dataset_py)
    
    # è·å–ä¸€ä¸ªbatch
    batch = next(iter(dataloader))
    
    # éªŒè¯batchæ ¼å¼
    print(f"âœ… Batch type: {type(batch)}")
    print(f"âœ… Batch length: {len(batch)}")
    print(f"âœ… Sample keys: {batch[0].keys()}")
    
    # æ£€æŸ¥å¿…éœ€å­—æ®µ
    required_keys = ["image", "lang", "action"]
    for key in required_keys:
        assert key in batch[0], f"Missing key: {key}"
    
    # æ‰“å°æ ·æœ¬ä¿¡æ¯
    sample = batch[0]
    print(f"\nğŸ“Š Sample Info:")
    print(f"  - Images: {len(sample['image'])} views, shape: {sample['image'][0].size}")
    print(f"  - Language: {sample['lang'][:100]}...")  # å‰100å­—ç¬¦
    print(f"  - Action shape: {np.array(sample['action']).shape}")
    if "state" in sample:
        print(f"  - State shape: {np.array(sample['state']).shape}")
    
    # æ£€æŸ¥ @ åˆ†ç•Œç¬¦
    if " @ " in sample['lang']:
        print(f"âœ… Found @ delimiter in language")
        parts = sample['lang'].split(" @ ", 1)
        print(f"  - Instruction part: {parts[0][:50]}...")
        print(f"  - Reasoning part: {parts[1][:50]}...")
    
    # æ£€æŸ¥thinking tokens (stage 2+)
    if "<|thinking|>" in sample['lang']:
        print(f"âœ… Found thinking tokens in language")
        print(f"  - <|start_of_thinking|>: {'Yes' if '<|start_of_thinking|>' in sample['lang'] else 'No'}")
        print(f"  - <|end_of_thinking|>: {'Yes' if '<|end_of_thinking|>' in sample['lang'] else 'No'}")
    
    return dataloader
```

#### æ¨¡å—4: æ¨¡å‹æ„å»ºæµ‹è¯•

```python
def test_model_build(cfg):
    """
    æµ‹è¯•æ¨¡å‹æ„å»º
    
    éªŒè¯:
    1. æ¨¡å‹å¯ä»¥æ­£å¸¸åˆ›å»º
    2. Thinking tokensæ­£ç¡®æ·»åŠ åˆ°tokenizer
    3. æ¨¡å‹å¯ä»¥ç§»åŠ¨åˆ°GPU
    4. å‚æ•°æ•°é‡åˆç†
    """
    print("\n" + "="*80)
    print("ğŸ—ï¸  Testing Model Build")
    print("="*80)
    
    # æ„å»ºæ¨¡å‹
    model = build_framework(cfg)
    
    # æ£€æŸ¥thinking tokens
    if cfg.framework.enable_latent_reasoning:
        tokenizer = model.qwen_vl_interface.processor.tokenizer
        vocab = tokenizer.get_vocab()
        
        thinking_tokens = [
            "<|thinking|>",
            "<|start_of_thinking|>",
            "<|end_of_thinking|>"
        ]
        
        for token in thinking_tokens:
            if token in vocab:
                print(f"âœ… {token}: ID={vocab[token]}")
            else:
                print(f"âŒ {token}: NOT FOUND")
    
    # ç§»åŠ¨åˆ°GPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    print(f"âœ… Model moved to {device}")
    
    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"âœ… Total parameters: {total_params:,}")
    print(f"âœ… Trainable parameters: {trainable_params:,}")
    
    return model
```

#### æ¨¡å—5: å‰å‘ä¼ æ’­æµ‹è¯•ï¼ˆæ— æ¢¯åº¦ï¼‰

```python
def test_forward_pass(model, dataloader, cfg):
    """
    æµ‹è¯•å‰å‘ä¼ æ’­ï¼ˆæ— æ¢¯åº¦ï¼‰
    
    éªŒè¯:
    1. build_qwenvl_inputsæ­£ç¡®æ„å»ºè¾“å…¥
    2. è¾“å…¥åŒ…å«å¿…éœ€å­—æ®µï¼ˆinput_ids, attention_mask, pixel_values, labels, position_idsï¼‰
    3. Forwardå¯ä»¥æ­£å¸¸æ‰§è¡Œ
    4. è¾“å‡ºåŒ…å«å¿…éœ€å­—æ®µï¼ˆaction_loss, vlm_loss, total_lossï¼‰
    5. Losså€¼åˆç†ï¼ˆä¸ä¸ºNaN/Infï¼‰
    6. Hidden states shapeæ­£ç¡®
    """
    print("\n" + "="*80)
    print("ğŸ”¬ Testing Forward Pass (No Gradient)")
    print("="*80)
    
    model.eval()
    batch = next(iter(dataloader))
    
    with torch.no_grad():
        # Step 1: æµ‹è¯• build_qwenvl_inputs
        print("\nğŸ“ Step 1: Testing build_qwenvl_inputs")
        batch_images = [example["image"] for example in batch]
        instructions = [example["lang"] for example in batch]
        
        qwen_inputs = model.qwen_vl_interface.build_qwenvl_inputs(
            images=batch_images,
            instructions=instructions
        )
        
        print(f"âœ… Input keys: {qwen_inputs.keys()}")
        print(f"âœ… input_ids shape: {qwen_inputs['input_ids'].shape}")
        print(f"âœ… attention_mask shape: {qwen_inputs['attention_mask'].shape}")
        if "position_ids" in qwen_inputs:
            print(f"âœ… position_ids shape: {qwen_inputs['position_ids'].shape}")
        if "labels" in qwen_inputs:
            print(f"âœ… labels shape: {qwen_inputs['labels'].shape}")
            # æ£€æŸ¥maskæ¯”ä¾‹
            total_tokens = qwen_inputs['labels'].numel()
            masked_tokens = (qwen_inputs['labels'] == -100).sum().item()
            print(f"âœ… Masked tokens: {masked_tokens}/{total_tokens} ({masked_tokens/total_tokens*100:.1f}%)")
        
        # æ£€æŸ¥thinking tokenå¯¹é½ï¼ˆå¦‚æœæœ‰ï¼‰
        if cfg.framework.enable_latent_reasoning:
            thinking_token_id = getattr(model.qwen_vl_interface, "thinking_token_id", None)
            if thinking_token_id is not None:
                # æ‰¾åˆ°æ¯ä¸ªæ ·æœ¬çš„ç¬¬ä¸€ä¸ªthinking tokenä½ç½®
                B = qwen_inputs['input_ids'].shape[0]
                first_thinking_positions = []
                for b in range(B):
                    ids = qwen_inputs['input_ids'][b]
                    thinking_mask = (ids == thinking_token_id)
                    if thinking_mask.any():
                        pos = thinking_mask.nonzero()[0].item()
                        first_thinking_positions.append(pos)
                
                if first_thinking_positions:
                    print(f"âœ… First thinking token positions: {first_thinking_positions}")
                    if len(set(first_thinking_positions)) == 1:
                        print(f"âœ… Thinking tokens are ALIGNED!")
                    else:
                        print(f"âš ï¸  Thinking tokens are NOT aligned")
        
        # Step 2: æµ‹è¯•å®Œæ•´forward
        print("\nğŸš€ Step 2: Testing full forward")
        output_dict = model.forward(batch)
        
        print(f"âœ… Output keys: {output_dict.keys()}")
        
        # æ£€æŸ¥loss
        for loss_name in ["action_loss", "vlm_loss", "total_loss"]:
            if loss_name in output_dict:
                loss_value = output_dict[loss_name]
                print(f"âœ… {loss_name}: {loss_value.item():.4f}")
                assert not torch.isnan(loss_value), f"{loss_name} is NaN!"
                assert not torch.isinf(loss_value), f"{loss_name} is Inf!"
        
        # æ£€æŸ¥forwardç±»å‹ï¼ˆstage 0 vs stage 2+ï¼‰
        scheduled_stage = cfg.datasets.vla_data.ecot.scheduled_stage
        if scheduled_stage == 0:
            print(f"âœ… Stage 0: Should use normal forward")
        else:
            print(f"âœ… Stage {scheduled_stage}: Should use forward_latent with KV-Cache")
    
    model.train()
    return output_dict
```

#### æ¨¡å—6: è®­ç»ƒå¾ªç¯æµ‹è¯•ï¼ˆæœ‰æ¢¯åº¦ï¼‰

```python
def test_training_loop(model, dataloader, cfg):
    """
    æµ‹è¯•è®­ç»ƒå¾ªç¯
    
    éªŒè¯:
    1. å¯ä»¥æ­£å¸¸backward
    2. æ¢¯åº¦ä¸ä¸ºNaN/Inf
    3. å‚æ•°å¯ä»¥æ›´æ–°
    4. å¤šæ­¥è®­ç»ƒç¨³å®š
    5. Lossè¶‹åŠ¿åˆç†
    """
    print("\n" + "="*80)
    print("ğŸ‹ï¸  Testing Training Loop")
    print("="*80)
    
    model.train()
    
    # åˆ›å»ºoptimizer
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=cfg.trainer.learning_rate.base,
        betas=tuple(cfg.trainer.optimizer.betas),
        weight_decay=cfg.trainer.optimizer.weight_decay,
        eps=cfg.trainer.optimizer.eps,
    )
    
    # è®°å½•åˆå§‹å‚æ•°ï¼ˆç”¨äºéªŒè¯æ›´æ–°ï¼‰
    initial_param = None
    for name, param in model.named_parameters():
        if param.requires_grad:
            initial_param = (name, param.clone().detach())
            break
    
    # è®­ç»ƒå¾ªç¯
    losses = []
    data_iter = iter(dataloader)
    
    for step in tqdm(range(cfg.trainer.max_train_steps), desc="Training"):
        # è·å–batch
        try:
            batch = next(data_iter)
        except StopIteration:
            data_iter = iter(dataloader)
            batch = next(data_iter)
        
        # Forward
        optimizer.zero_grad()
        output_dict = model.forward(batch)
        
        # ä½¿ç”¨total_lossï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œå¦åˆ™ç”¨action_loss
        loss = output_dict.get("total_loss", output_dict["action_loss"])
        
        # Backward
        loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦
        has_nan_grad = False
        has_inf_grad = False
        grad_norms = []
        for name, param in model.named_parameters():
            if param.grad is not None:
                if torch.isnan(param.grad).any():
                    has_nan_grad = True
                    print(f"âš ï¸  NaN gradient in {name}")
                if torch.isinf(param.grad).any():
                    has_inf_grad = True
                    print(f"âš ï¸  Inf gradient in {name}")
                grad_norms.append(param.grad.norm().item())
        
        if has_nan_grad or has_inf_grad:
            print(f"âŒ Step {step}: Gradient check FAILED")
            break
        
        # Gradient clipping
        if cfg.trainer.gradient_clipping is not None:
            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.trainer.gradient_clipping)
        
        # Optimizer step
        optimizer.step()
        
        # è®°å½•loss
        losses.append(loss.item())
        
        # æ¯æ­¥æ‰“å°
        if step % cfg.trainer.logging_frequency == 0:
            log_str = f"Step {step}: "
            if "action_loss" in output_dict:
                log_str += f"action_loss={output_dict['action_loss'].item():.4f} "
            if "vlm_loss" in output_dict:
                log_str += f"vlm_loss={output_dict['vlm_loss'].item():.4f} "
            log_str += f"total_loss={loss.item():.4f}"
            print(log_str)
    
    # éªŒè¯å‚æ•°æ›´æ–°
    if initial_param is not None:
        name, initial_value = initial_param
        current_value = dict(model.named_parameters())[name]
        param_changed = not torch.equal(initial_value, current_value)
        if param_changed:
            print(f"âœ… Parameters updated (checked: {name})")
        else:
            print(f"âš ï¸  Parameters NOT updated (checked: {name})")
    
    # Lossè¶‹åŠ¿
    print(f"\nğŸ“Š Loss Statistics:")
    print(f"  - Initial loss: {losses[0]:.4f}")
    print(f"  - Final loss: {losses[-1]:.4f}")
    print(f"  - Mean loss: {np.mean(losses):.4f}")
    print(f"  - Std loss: {np.std(losses):.4f}")
    
    return losses
```

#### æ¨¡å—7: ä¸»å‡½æ•°

```python
def main():
    """
    ä¸»æµ‹è¯•æµç¨‹
    """
    # è§£æå‚æ•°
    parser = argparse.ArgumentParser(description="ECoT End-to-End Test Training")
    parser.add_argument("--config_yaml", type=str, required=True, help="Path to test config YAML")
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    cfg = OmegaConf.load(args.config_yaml)
    
    print("\n" + "="*80)
    print("ğŸš€ ECoT Implicit Reasoning End-to-End Test")
    print("="*80)
    print(f"Config: {args.config_yaml}")
    print(f"Stage: {cfg.datasets.vla_data.ecot.scheduled_stage}")
    
    # æµ‹è¯•æµç¨‹
    try:
        # 1. éªŒè¯é…ç½®
        validate_config(cfg)
        
        # 2. æµ‹è¯•æ•°æ®åŠ è½½
        dataloader = test_dataloader(cfg)
        
        # 3. æµ‹è¯•æ¨¡å‹æ„å»º
        model = test_model_build(cfg)
        
        # 4. æµ‹è¯•å‰å‘ä¼ æ’­ï¼ˆæ— æ¢¯åº¦ï¼‰
        test_forward_pass(model, dataloader, cfg)
        
        # 5. æµ‹è¯•è®­ç»ƒå¾ªç¯ï¼ˆæœ‰æ¢¯åº¦ï¼‰
        losses = test_training_loop(model, dataloader, cfg)
        
        # 6. æœ€ç»ˆæŠ¥å‘Š
        print("\n" + "="*80)
        print("âœ… ALL TESTS PASSED!")
        print("="*80)
        print(f"âœ… Data loading: OK")
        print(f"âœ… Model building: OK")
        print(f"âœ… Forward pass: OK")
        print(f"âœ… Training loop: OK")
        print(f"âœ… Gradient flow: OK")
        print(f"âœ… Parameter update: OK")
        print("\nğŸ‰ ECoT pipeline is ready for full training!")
        
    except Exception as e:
        print("\n" + "="*80)
        print("âŒ TEST FAILED")
        print("="*80)
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
```

---

## å››ã€æµ‹è¯•æ‰§è¡Œè®¡åˆ’

### 4.1 æµ‹è¯•é¡ºåº

```bash
# Step 1: æµ‹è¯• Stage 0 (æ— thinking tokens)
python test_ecot_training.py --config_yaml config/test_ecot_stage0.yaml

# Step 2: æµ‹è¯• Stage 2 (æœ‰thinking tokens)
python test_ecot_training.py --config_yaml config/test_ecot_stage2.yaml
```

### 4.2 é¢„æœŸè¾“å‡º

#### Stage 0 é¢„æœŸ
```
âœ… Found @ delimiter in language
âœ… Masked tokens: 450/512 (87.9%)  # Instructionè¢«mask
âœ… Stage 0: Should use normal forward
âœ… action_loss: 2.3456
âœ… vlm_loss: 1.2345
âœ… total_loss: 2.4689
âœ… Parameters updated
```

#### Stage 2 é¢„æœŸ
```
âœ… Found @ delimiter in language
âœ… Found thinking tokens in language
âœ… Thinking tokens are ALIGNED!
âœ… Masked tokens: 480/512 (93.8%)  # Instruction + latentè¢«mask
âœ… Stage 2: Should use forward_latent with KV-Cache
âœ… action_loss: 2.3456
âœ… vlm_loss: 1.2345
âœ… total_loss: 2.4689
âœ… Parameters updated
```

### 4.3 å¤±è´¥è¯Šæ–­

| é”™è¯¯ç°è±¡ | å¯èƒ½åŸå›  | è§£å†³æ–¹æ¡ˆ |
|---------|---------|---------|
| `@ delimiter not found` | æ•°æ®å¤„ç†æœªæ·»åŠ  `@` | æ£€æŸ¥ `ECOTBatchTransform` |
| `Thinking tokens NOT aligned` | å¯¹é½é€»è¾‘é”™è¯¯ | æ£€æŸ¥ `_align_thinking_tokens` |
| `NaN loss` | Label maské”™è¯¯æˆ–æ•°å€¼ä¸ç¨³å®š | æ£€æŸ¥ `_build_ecot_labels_batch` |
| `NaN gradient` | Forwardè®¡ç®—é”™è¯¯ | æ£€æŸ¥ `forward_latent` |
| `Parameters NOT updated` | æ‰€æœ‰å‚æ•°è¢«å†»ç»“ | æ£€æŸ¥ `requires_grad` |
| `OOM` | Batch sizeå¤ªå¤§æˆ–åºåˆ—å¤ªé•¿ | å‡å°batch sizeæˆ–model_max_length |

---

## äº”ã€æˆåŠŸæ ‡å‡†

### å¿…é¡»é€šè¿‡çš„æ£€æŸ¥é¡¹

#### æ•°æ®å±‚é¢
- [ ] DataLoaderå¯ä»¥æ­£å¸¸åˆ›å»ºå’Œè¿­ä»£
- [ ] Batchæ ¼å¼åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ
- [ ] `@` åˆ†ç•Œç¬¦å­˜åœ¨äºæ‰€æœ‰æ ·æœ¬
- [ ] Stage 2+çš„æ ·æœ¬åŒ…å«thinking tokens

#### Tokenizationå±‚é¢
- [ ] Thinking tokensæ­£ç¡®æ·»åŠ åˆ°tokenizer
- [ ] `build_qwenvl_inputs` è¿”å›æ‰€æœ‰å¿…éœ€å­—æ®µ
- [ ] `input_ids` å’Œ `attention_mask` shapeä¸€è‡´
- [ ] `position_ids` æ­£ç¡®ç”Ÿæˆ
- [ ] `labels` æ­£ç¡®ç”Ÿæˆä¸”åŒ…å«mask

#### å¯¹é½å±‚é¢
- [ ] Stage 2+çš„thinking tokensä½ç½®å¯¹é½
- [ ] å¯¹é½åçš„åºåˆ—é•¿åº¦åˆç†ï¼ˆä¸è¶…è¿‡model_max_lengthï¼‰

#### Forwardå±‚é¢
- [ ] Stage 0ä½¿ç”¨æ™®é€šforward
- [ ] Stage 2+ä½¿ç”¨ `forward_latent`
- [ ] æ‰€æœ‰losså€¼ä¸ºæœ‰é™æ•°ï¼ˆä¸æ˜¯NaN/Infï¼‰
- [ ] `action_loss` å’Œ `vlm_loss` éƒ½èƒ½è®¡ç®—
- [ ] `total_loss` æ­£ç¡®åˆå¹¶

#### è®­ç»ƒå±‚é¢
- [ ] Backwardæ­£å¸¸æ‰§è¡Œ
- [ ] æ¢¯åº¦ä¸åŒ…å«NaN/Inf
- [ ] å‚æ•°å¯ä»¥æ›´æ–°
- [ ] è‡³å°‘èƒ½ç¨³å®šè®­ç»ƒ10æ­¥

---

## å…­ã€åç»­æ‰©å±•

### 6.1 å®Œæ•´è®­ç»ƒè„šæœ¬
æµ‹è¯•é€šè¿‡åï¼Œå¯ä»¥åŸºäº `train_starvla.py` åˆ›å»ºå®Œæ•´çš„ECoTè®­ç»ƒè„šæœ¬ï¼š
- æ·»åŠ åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
- æ·»åŠ checkpointä¿å­˜/æ¢å¤
- æ·»åŠ W&B logging
- æ·»åŠ evaluation

### 6.2 æ€§èƒ½ä¼˜åŒ–
- æ·»åŠ mixed precisionè®­ç»ƒ
- ä¼˜åŒ–DataLoaderï¼ˆå¢åŠ num_workersï¼‰
- æ·»åŠ gradient checkpointing

### 6.3 å¤šé˜¶æ®µè®­ç»ƒ
- å®ç°curriculum learningï¼ˆstage 0 â†’ 1 â†’ 2 â†’ ...ï¼‰
- è‡ªåŠ¨åˆ‡æ¢scheduled_stage
- ä¿å­˜æ¯ä¸ªstageçš„checkpoint

---

## ä¸ƒã€æ£€æŸ¥æ¸…å•

åœ¨è¿è¡Œæµ‹è¯•å‰ï¼Œç¡®è®¤ï¼š
- [ ] æ•°æ®è·¯å¾„æ­£ç¡®ä¸”æ•°æ®å­˜åœ¨
- [ ] Reasoning JSONæ–‡ä»¶å­˜åœ¨
- [ ] GPUå¯ç”¨ä¸”å†…å­˜è¶³å¤Ÿï¼ˆå»ºè®®>=16GBï¼‰
- [ ] å·²å®‰è£…æ‰€æœ‰ä¾èµ–
- [ ] HF_ENDPOINTå’ŒHF_HOMEå·²è®¾ç½®
- [ ] Qwen3-VLæ¨¡å‹å¯ä»¥ä¸‹è½½æˆ–å·²ç¼“å­˜

åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­ï¼Œè§‚å¯Ÿï¼š
- [ ] æ•°æ®åŠ è½½æ—¶é—´åˆç†ï¼ˆ<10s per batchï¼‰
- [ ] æ¨¡å‹åŠ è½½æ—¶é—´åˆç†ï¼ˆ<2minï¼‰
- [ ] Forwardæ—¶é—´åˆç†ï¼ˆStage 0: <2s, Stage 2: <5s per batchï¼‰
- [ ] å†…å­˜ä½¿ç”¨ç¨³å®šï¼ˆä¸æŒç»­å¢é•¿ï¼‰
- [ ] Losså€¼åœ¨åˆç†èŒƒå›´ï¼ˆ0.1-10.0ï¼‰

æµ‹è¯•é€šè¿‡åï¼Œç¡®è®¤ï¼š
- [ ] æ‰€æœ‰æµ‹è¯•æ¨¡å—éƒ½æ‰“å°äº† âœ…
- [ ] æ²¡æœ‰ âš ï¸ æˆ– âŒ è¾“å‡º
- [ ] Lossæ›²çº¿å¹³æ»‘ï¼ˆæ— çªå˜ï¼‰
- [ ] å¯ä»¥é‡å¤è¿è¡Œæµ‹è¯•ï¼ˆç»“æœä¸€è‡´ï¼‰

---

## å…«ã€æ€»ç»“

æœ¬æµ‹è¯•è„šæœ¬è®¾è®¡ä¸º**æ¸è¿›å¼éªŒè¯**ï¼Œä»æ•°æ®åˆ°æ¨¡å‹åˆ°è®­ç»ƒï¼Œé€å±‚æµ‹è¯•ã€‚æ¯ä¸ªæ¨¡å—ç‹¬ç«‹ä¸”å¯è°ƒè¯•ï¼Œå¤±è´¥æ—¶å¯ä»¥å¿«é€Ÿå®šä½é—®é¢˜ã€‚

**è®¾è®¡åŸåˆ™**ï¼š
1. **ä¿å®ˆå‚æ•°**ï¼šå°batch sizeã€çŸ­åºåˆ—ã€å°‘è®­ç»ƒæ­¥æ•°
2. **è¯¦ç»†è¾“å‡º**ï¼šæ¯ä¸ªæ­¥éª¤éƒ½æœ‰æ˜ç¡®çš„ âœ…/âŒ æ ‡è®°
3. **å¿«é€Ÿå¤±è´¥**ï¼šå‘ç°é—®é¢˜ç«‹å³åœæ­¢å¹¶æŠ¥å‘Š
4. **å¯é‡å¤**ï¼šå›ºå®šseedï¼Œç»“æœå¯å¤ç°
5. **ç‹¬ç«‹æ¨¡å—**ï¼šæ¯ä¸ªæµ‹è¯•å‡½æ•°å¯ä»¥å•ç‹¬è¿è¡Œ

**é¢„æœŸæ—¶é—´**ï¼š
- Stage 0æµ‹è¯•ï¼š~5-10åˆ†é’Ÿ
- Stage 2æµ‹è¯•ï¼š~10-15åˆ†é’Ÿï¼ˆKV-Cacheå¤šæ¬¡forwardæ›´æ…¢ï¼‰

æµ‹è¯•é€šè¿‡åï¼Œå³å¯å¼€å§‹å®Œæ•´çš„è®­ç»ƒå®éªŒï¼

