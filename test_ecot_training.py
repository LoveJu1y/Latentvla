"""
ECoT Implicit Reasoning End-to-End Test Training Script

Purpose: Validate the entire pipeline from data loading to training
- Stage 0: No thinking tokens, @ delimiter only
- Stage 2+: With thinking tokens, KV-Cache iterative forward

Usage:
    python test_ecot_training.py --config_yaml config/test_ecot_stage0.yaml
    python test_ecot_training.py --config_yaml config/test_ecot_stage2.yaml
"""

import argparse
import os
import sys
from pathlib import Path
import torch
import numpy as np
from omegaconf import OmegaConf
from tqdm import tqdm

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¿…é¡»åœ¨å¯¼å…¥å…¶ä»–æ¨¡å—ä¹‹å‰ï¼‰
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["HF_HOME"] = "./qwen"

# Local imports (å»¶è¿Ÿå¯¼å…¥ä»¥é¿å…ä¸å¿…è¦çš„ä¾èµ–)
from starVLA.model.framework import build_framework


def validate_config(cfg):
    """
    éªŒè¯é…ç½®æ–‡ä»¶çš„å®Œæ•´æ€§å’Œåˆç†æ€§
    
    æ£€æŸ¥é¡¹:
    1. å¿…éœ€å­—æ®µå­˜åœ¨
    2. è·¯å¾„æœ‰æ•ˆ
    3. scheduled_stageä¸enable_latent_reasoningä¸€è‡´
    4. batch_sizeåˆç†ï¼ˆå»ºè®®<=4ï¼‰
    """
    print("\n" + "="*80)
    print("ğŸ“‹ Configuration Validation")
    print("="*80)
    
    # æ£€æŸ¥å¿…éœ€å­—æ®µ
    required_fields = {
        "datasets.vla_data.dataset_py": cfg.datasets.vla_data.dataset_py,
        "datasets.vla_data.ecot.data_root_dir": cfg.datasets.vla_data.ecot.data_root_dir,
        "datasets.vla_data.ecot.scheduled_stage": cfg.datasets.vla_data.ecot.scheduled_stage,
        "framework.name": cfg.framework.name,
        "framework.enable_latent_reasoning": cfg.framework.enable_latent_reasoning,
    }
    
    print("âœ… Required fields check:")
    for field_name, field_value in required_fields.items():
        print(f"  - {field_name}: {field_value}")
    
    # æ£€æŸ¥æ•°æ®è·¯å¾„
    data_root = cfg.datasets.vla_data.ecot.data_root_dir
    reasoning_json = cfg.datasets.vla_data.ecot.reasoning_json
    
    if not os.path.exists(data_root):
        print(f"âš ï¸  Warning: data_root_dir does not exist: {data_root}")
    else:
        print(f"âœ… Data root exists: {data_root}")
    
    if not os.path.exists(reasoning_json):
        print(f"âš ï¸  Warning: reasoning_json does not exist: {reasoning_json}")
    else:
        print(f"âœ… Reasoning JSON exists: {reasoning_json}")
    
    # æ£€æŸ¥batch size
    batch_size = cfg.datasets.vla_data.per_device_batch_size
    if batch_size > 4:
        print(f"âš ï¸  Warning: batch_size={batch_size} is large for testing, recommend <=4")
    else:
        print(f"âœ… Batch size is reasonable: {batch_size}")
    
    # æ‰“å°å…³é”®é…ç½®
    print(f"\nğŸ“Š Key Configuration:")
    print(f"  - Stage: {cfg.datasets.vla_data.ecot.scheduled_stage}")
    print(f"  - Enable Latent Reasoning: {cfg.framework.enable_latent_reasoning}")
    print(f"  - Compute Language Loss: {cfg.framework.latent_reasoning.compute_language_loss}")
    print(f"  - Batch Size: {batch_size}")
    print(f"  - Max Steps: {cfg.trainer.max_train_steps}")
    print(f"  - Model: {cfg.framework.qwenvl.base_vlm}")
    
    return True


def test_dataloader(cfg):
    """
    æµ‹è¯•æ•°æ®åŠ è½½å’Œæ ¼å¼
    
    éªŒè¯:
    1. DataLoaderå¯ä»¥æ­£å¸¸åˆ›å»º
    2. å¯ä»¥è·å–batch
    3. Batchæ ¼å¼æ­£ç¡®ï¼ˆåŒ…å«å¿…éœ€å­—æ®µï¼‰
    4. æ•°æ®ç±»å‹å’Œshapeæ­£ç¡®
    """
    print("\n" + "="*80)
    print("ğŸ“¦ Testing DataLoader")
    print("="*80)
    
    # ç›´æ¥ä½¿ç”¨ ECOT RLDS çš„ dataloader builder
    from starVLA.integrations.ecot_rlds.builder import make_dataloader_ecot
    
    # åˆ›å»ºdataloader
    print("Creating ECOT RLDS dataloader...")
    dataloader = make_dataloader_ecot(cfg)
    print(f"âœ… DataLoader created successfully")
    
    # è·å–ä¸€ä¸ªbatch
    print("\nFetching first batch...")
    batch = next(iter(dataloader))
    
    # éªŒè¯batchæ ¼å¼
    print(f"âœ… Batch type: {type(batch)}")
    print(f"âœ… Batch length (samples): {len(batch)}")
    print(f"âœ… Sample keys: {list(batch[0].keys())}")
    
    # æ£€æŸ¥å¿…éœ€å­—æ®µ
    required_keys = ["image", "lang", "action"]
    for key in required_keys:
        if key not in batch[0]:
            print(f"âŒ Missing required key: {key}")
            return None
        else:
            print(f"âœ… Found required key: {key}")
    
    # æ‰“å°æ ·æœ¬ä¿¡æ¯
    sample = batch[0]
    print(f"\nğŸ“Š Sample Info:")
    print(f"  - Images: {len(sample['image'])} views")
    if len(sample['image']) > 0:
        print(f"    - Image 0 size: {sample['image'][0].size}")
        print(f"    - Image 0 mode: {sample['image'][0].mode}")
    
    lang_preview = sample['lang'][:100] if len(sample['lang']) > 100 else sample['lang']
    print(f"  - Language (first 100 chars): {lang_preview}...")
    print(f"  - Language full length: {len(sample['lang'])} chars")
    
    action_array = np.array(sample['action'])
    print(f"  - Action shape: {action_array.shape}")
    print(f"  - Action dtype: {action_array.dtype}")
    
    if "state" in sample:
        state_array = np.array(sample['state'])
        print(f"  - State shape: {state_array.shape}")
        print(f"  - State dtype: {state_array.dtype}")
    
    # æ£€æŸ¥ @ åˆ†ç•Œç¬¦
    print(f"\nğŸ” Checking @ delimiter:")
    if " @ " in sample['lang']:
        print(f"âœ… Found @ delimiter in language")
        parts = sample['lang'].split(" @ ", 1)
        instr_preview = parts[0][:50] if len(parts[0]) > 50 else parts[0]
        reason_preview = parts[1][:50] if len(parts[1]) > 50 else parts[1]
        print(f"  - Instruction part (first 50 chars): {instr_preview}...")
        print(f"  - Reasoning part (first 50 chars): {reason_preview}...")
    else:
        print(f"âš ï¸  @ delimiter NOT found in language")
        print(f"  - Full language text: {sample['lang'][:200]}...")
    
    # æ£€æŸ¥ reasoning å­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if "reasoning" in sample:
        reasoning = sample['reasoning']
        if reasoning:
            print(f"âœ… Reasoning field exists and non-empty")
            print(f"  - Reasoning (first 100 chars): {reasoning[:100]}...")
        else:
            print(f"âš ï¸  Reasoning field exists but is EMPTY")
    else:
        print(f"âš ï¸  Reasoning field NOT in sample")
    
    # æ£€æŸ¥thinking tokens (stage 2+)
    print(f"\nğŸ” Checking thinking tokens:")
    has_thinking = "<|thinking|>" in sample['lang']
    has_start = "<|start_of_thinking|>" in sample['lang']
    has_end = "<|end_of_thinking|>" in sample['lang']
    
    if has_thinking or has_start or has_end:
        print(f"âœ… Found thinking tokens in language")
        print(f"  - <|thinking|>: {'Yes' if has_thinking else 'No'}")
        print(f"  - <|start_of_thinking|>: {'Yes' if has_start else 'No'}")
        print(f"  - <|end_of_thinking|>: {'Yes' if has_end else 'No'}")
    else:
        scheduled_stage = cfg.datasets.vla_data.ecot.scheduled_stage
        if scheduled_stage == 0:
            print(f"âœ… No thinking tokens found (expected for stage 0)")
        else:
            print(f"âš ï¸  No thinking tokens found (unexpected for stage {scheduled_stage})")
    
    return dataloader


def test_model_build(cfg):
    """
    æµ‹è¯•æ¨¡å‹æ„å»º
    
    éªŒè¯:
    1. æ¨¡å‹å¯ä»¥æ­£å¸¸åˆ›å»º
    2. Thinking tokensæ­£ç¡®æ·»åŠ åˆ°tokenizer
    3. æ¨¡å‹å¯ä»¥ç§»åŠ¨åˆ°GPU
    4. å‚æ•°æ•°é‡åˆç†
    """
    print("\n" + "="*80)
    print("ğŸ—ï¸  Testing Model Build")
    print("="*80)
    
    # æ„å»ºæ¨¡å‹
    print("Building model...")
    model = build_framework(cfg)
    print(f"âœ… Model built successfully: {type(model).__name__}")
    
    # æ£€æŸ¥thinking tokens
    if cfg.framework.enable_latent_reasoning:
        print(f"\nğŸ” Checking thinking tokens in tokenizer:")
        tokenizer = model.qwen_vl_interface.processor.tokenizer
        vocab = tokenizer.get_vocab()
        
        thinking_tokens = [
            "<|thinking|>",
            "<|start_of_thinking|>",
            "<|end_of_thinking|>"
        ]
        
        for token in thinking_tokens:
            if token in vocab:
                token_id = vocab[token]
                print(f"âœ… {token}: ID={token_id}")
            else:
                print(f"âŒ {token}: NOT FOUND in vocabulary")
    else:
        print(f"â„¹ï¸  Latent reasoning disabled, skipping thinking token check")
    
    # ç§»åŠ¨åˆ°GPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\nğŸ”§ Moving model to device: {device}")
    model = model.to(device)
    print(f"âœ… Model moved to {device}")
    
    # ç»Ÿè®¡å‚æ•°
    print(f"\nğŸ“Š Model Statistics:")
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    frozen_params = total_params - trainable_params
    
    print(f"  - Total parameters: {total_params:,}")
    print(f"  - Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)")
    print(f"  - Frozen parameters: {frozen_params:,} ({frozen_params/total_params*100:.1f}%)")
    
    # æ£€æŸ¥ä¸»è¦ç»„ä»¶
    print(f"\nğŸ” Model Components:")
    print(f"  - QWen VL Interface: {type(model.qwen_vl_interface).__name__}")
    print(f"  - Action Model: {type(model.action_model).__name__}")
    
    return model


def test_forward_pass(model, dataloader, cfg):
    """
    æµ‹è¯•å‰å‘ä¼ æ’­ï¼ˆæ— æ¢¯åº¦ï¼‰
    
    éªŒè¯:
    1. build_qwenvl_inputsæ­£ç¡®æ„å»ºè¾“å…¥
    2. è¾“å…¥åŒ…å«å¿…éœ€å­—æ®µï¼ˆinput_ids, attention_mask, pixel_values, labels, position_idsï¼‰
    3. Forwardå¯ä»¥æ­£å¸¸æ‰§è¡Œ
    4. è¾“å‡ºåŒ…å«å¿…éœ€å­—æ®µï¼ˆaction_loss, vlm_loss, total_lossï¼‰
    5. Losså€¼åˆç†ï¼ˆä¸ä¸ºNaN/Infï¼‰
    6. Hidden states shapeæ­£ç¡®
    """
    print("\n" + "="*80)
    print("ğŸ”¬ Testing Forward Pass (No Gradient)")
    print("="*80)
    
    model.eval()
    batch = next(iter(dataloader))
    
    with torch.no_grad():
        # Step 1: æµ‹è¯• build_qwenvl_inputs
        print("\nğŸ“ Step 1: Testing build_qwenvl_inputs")
        batch_images = [example["image"] for example in batch]
        instructions = [example["lang"] for example in batch]
        
        print(f"  - Building inputs for {len(batch)} samples...")
        qwen_inputs = model.qwen_vl_interface.build_qwenvl_inputs(
            images=batch_images,
            instructions=instructions
        )
        
        print(f"âœ… Input keys: {list(qwen_inputs.keys())}")
        print(f"âœ… input_ids shape: {qwen_inputs['input_ids'].shape}")
        print(f"âœ… attention_mask shape: {qwen_inputs['attention_mask'].shape}")
        
        if "position_ids" in qwen_inputs:
            print(f"âœ… position_ids shape: {qwen_inputs['position_ids'].shape}")
        else:
            print(f"â„¹ï¸  position_ids not in inputs (may be generated internally)")
        
        if "labels" in qwen_inputs:
            print(f"âœ… labels shape: {qwen_inputs['labels'].shape}")
            # æ£€æŸ¥maskæ¯”ä¾‹
            total_tokens = qwen_inputs['labels'].numel()
            masked_tokens = (qwen_inputs['labels'] == -100).sum().item()
            trainable_tokens = total_tokens - masked_tokens
            print(f"âœ… Label statistics:")
            print(f"  - Total tokens: {total_tokens}")
            print(f"  - Masked tokens (IGNORE_INDEX): {masked_tokens} ({masked_tokens/total_tokens*100:.1f}%)")
            print(f"  - Trainable tokens: {trainable_tokens} ({trainable_tokens/total_tokens*100:.1f}%)")
        else:
            print(f"â„¹ï¸  labels not in inputs (may not compute VLM loss)")
        
        if "pixel_values" in qwen_inputs:
            pv = qwen_inputs['pixel_values']
            if isinstance(pv, dict):
                print(f"âœ… pixel_values (dict): {list(pv.keys())}")
                for k, v in pv.items():
                    print(f"  - {k}: shape={v.shape}, dtype={v.dtype}")
            else:
                print(f"âœ… pixel_values: shape={pv.shape}, dtype={pv.dtype}")
        
        # æ£€æŸ¥thinking tokenå¯¹é½ï¼ˆå¦‚æœæœ‰ï¼‰
        if cfg.framework.enable_latent_reasoning:
            print(f"\nğŸ” Checking thinking token alignment:")
            thinking_token_id = getattr(model.qwen_vl_interface, "thinking_token_id", None)
            if thinking_token_id is not None:
                print(f"  - Thinking token ID: {thinking_token_id}")
                # æ‰¾åˆ°æ¯ä¸ªæ ·æœ¬çš„ç¬¬ä¸€ä¸ªthinking tokenä½ç½®
                B = qwen_inputs['input_ids'].shape[0]
                first_thinking_positions = []
                for b in range(B):
                    ids = qwen_inputs['input_ids'][b]
                    thinking_mask = (ids == thinking_token_id)
                    if thinking_mask.any():
                        pos = thinking_mask.nonzero()[0].item()
                        first_thinking_positions.append(pos)
                        # ç»Ÿè®¡thinking tokenæ•°é‡
                        count = thinking_mask.sum().item()
                        print(f"  - Sample {b}: first position={pos}, total count={count}")
                    else:
                        print(f"  - Sample {b}: no thinking tokens found")
                
                if first_thinking_positions:
                    if len(set(first_thinking_positions)) == 1:
                        print(f"âœ… Thinking tokens are ALIGNED at position {first_thinking_positions[0]}!")
                    else:
                        print(f"âš ï¸  Thinking tokens are NOT aligned: positions={first_thinking_positions}")
            else:
                print(f"â„¹ï¸  thinking_token_id not found in model")
        
        # æ£€æŸ¥label maskçš„æ­£ç¡®æ€§ï¼ˆInstructionå’ŒLatentæ˜¯å¦è¢«æ­£ç¡®maskï¼‰
        if "labels" in qwen_inputs:
            print(f"\nğŸ” Checking label mask correctness:")
            labels = qwen_inputs['labels']
            input_ids = qwen_inputs['input_ids']
            pad_id = model.qwen_vl_interface.processor.tokenizer.pad_token_id
            IGNORE_INDEX = -100
            
            # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ ·æœ¬çš„maskæƒ…å†µ
            sample_idx = 0
            sample_labels = labels[sample_idx]
            sample_ids = input_ids[sample_idx]
            valid_mask = (sample_ids != pad_id)
            valid_length = valid_mask.sum().item()
            
            # æ‰¾åˆ°instructionå’Œlatentçš„è¾¹ç•Œ
            start_id = getattr(model.qwen_vl_interface, "start_thinking_id", None)
            end_id = getattr(model.qwen_vl_interface, "end_thinking_id", None)
            thinking_token_id = getattr(model.qwen_vl_interface, "thinking_token_id", None)
            
            # æ£€æŸ¥instructionæ®µæ˜¯å¦è¢«mask
            # å¯¹äºStage 2+: ä½¿ç”¨start_thinkingä½ç½®
            # å¯¹äºStage 0: ä½¿ç”¨@åˆ†ç•Œç¬¦ä½ç½®
            scheduled_stage = cfg.datasets.vla_data.ecot.scheduled_stage
            instr_end = None
            
            if start_id is not None:
                start_pos = (sample_ids == start_id).nonzero()
                if start_pos.numel() > 0:
                    instr_end = start_pos[0].item()
                    instr_masked = (sample_labels[:instr_end] == IGNORE_INDEX).all().item()
                    print(f"  - Instruction span [0:{instr_end}) (via <|start_of_thinking|>): {'âœ… MASKED' if instr_masked else 'âŒ NOT MASKED'}")
            
            # å¯¹äºStage 0ï¼Œæ£€æŸ¥@åˆ†ç•Œç¬¦
            if instr_end is None and scheduled_stage == 0:
                # å°è¯•æ‰¾åˆ°@åˆ†ç•Œç¬¦
                tokenizer = model.qwen_vl_interface.processor.tokenizer
                try:
                    at_token_ids = tokenizer.encode(" @ ", add_special_tokens=False)
                    print(f"at_token_ids: {at_token_ids}")
                    if at_token_ids:
                        # æŸ¥æ‰¾@åˆ†ç•Œç¬¦çš„ä½ç½®
                        at_tensor = torch.tensor(at_token_ids, device=sample_ids.device, dtype=sample_ids.dtype)
                        at_len = len(at_token_ids)
                        for i in range(len(sample_ids) - at_len + 1):
                            if torch.equal(sample_ids[i:i+at_len], at_tensor):
                                instr_end = i + at_len
                                instr_masked = (sample_labels[:instr_end] == IGNORE_INDEX).all().item()
                                print(f"  - Instruction span [0:{instr_end}) (via @ delimiter): {'âœ… MASKED' if instr_masked else 'âŒ NOT MASKED'}")
                                break
                except Exception as e:
                    print(f"  - âš ï¸  Could not check @ delimiter: {e}")
            
            # æ£€æŸ¥latentæ®µæ˜¯å¦è¢«maskï¼ˆå¦‚æœæœ‰thinking tokensï¼ŒStage 2+ï¼‰
            if thinking_token_id is not None and start_id is not None and end_id is not None and scheduled_stage > 0:
                start_pos = (sample_ids == start_id).nonzero()
                end_pos = (sample_ids == end_id).nonzero()
                if start_pos.numel() > 0 and end_pos.numel() > 0:
                    lat_start = start_pos[0].item()
                    lat_end = end_pos[0].item() + 1  # Include end token
                    latent_masked = (sample_labels[lat_start:lat_end] == IGNORE_INDEX).all().item()
                    print(f"  - Latent span [{lat_start}:{lat_end}): {'âœ… MASKED' if latent_masked else 'âŒ NOT MASKED'}")
                    
                    # æ£€æŸ¥post-latentæ®µæ˜¯å¦æœªè¢«mask
                    if lat_end < valid_length:
                        post_latent_labels = sample_labels[lat_end:valid_length]
                        post_latent_unmasked = (post_latent_labels != IGNORE_INDEX).any().item()
                        print(f"  - Post-latent span [{lat_end}:{valid_length}): {'âœ… UNMASKED (trainable)' if post_latent_unmasked else 'âš ï¸  ALL MASKED'}")
            elif scheduled_stage == 0:
                print(f"  - Stage 0: No latent span (no thinking tokens)")
        
        # Step 2: æµ‹è¯•å®Œæ•´forward
        print("\nğŸš€ Step 2: Testing full forward")
        print(f"  - Running forward pass...")
        print(f"  - Batch size: {len(batch)}")
        print(f"  - repeated_diffusion_steps: {cfg.trainer.repeated_diffusion_steps}")
        print(f"  - Expected effective batch: {len(batch) * cfg.trainer.repeated_diffusion_steps}")
        output_dict = model.forward(batch)
        
        print(f"âœ… Output keys: {list(output_dict.keys())}")
        
        # æ£€æŸ¥loss
        print(f"\nğŸ“Š Loss values:")
        for loss_name in ["action_loss", "vlm_loss", "total_loss"]:
            if loss_name in output_dict:
                loss_value = output_dict[loss_name]
                loss_item = loss_value.item()
                print(f"âœ… {loss_name}: {loss_item:.4f}")
                
                # æ£€æŸ¥æ•°å€¼æœ‰æ•ˆæ€§
                if torch.isnan(loss_value):
                    print(f"âŒ {loss_name} is NaN!")
                    raise ValueError(f"{loss_name} is NaN")
                if torch.isinf(loss_value):
                    print(f"âŒ {loss_name} is Inf!")
                    raise ValueError(f"{loss_name} is Inf")
            else:
                print(f"â„¹ï¸  {loss_name}: not in output")
        
        # æ£€æŸ¥forwardç±»å‹ï¼ˆstage 0 vs stage 2+ï¼‰
        scheduled_stage = cfg.datasets.vla_data.ecot.scheduled_stage
        print(f"\nğŸ” Forward type check:")
        if scheduled_stage == 0:
            print(f"âœ… Stage 0: Expected to use normal forward (or forward_latent with 0 passes)")
        else:
            print(f"âœ… Stage {scheduled_stage}: Expected to use forward_latent with KV-Cache")
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†iterative forward
        if "vlm_loss" in output_dict and output_dict["vlm_loss"] is not None:
            print(f"âœ… VLM loss computed, language model training is active")
        else:
            print(f"â„¹ï¸  VLM loss not computed or None")
        
        # æ£€æŸ¥forward_latentæ˜¯å¦è¢«è°ƒç”¨ï¼ˆé€šè¿‡æ£€æŸ¥forward_latentçš„è¿”å›å€¼ï¼‰
        # æ³¨æ„ï¼šè¿™éœ€è¦åœ¨forward_latentä¸­æ·»åŠ è¿”å›å€¼æ¥éªŒè¯ï¼Œç›®å‰åªèƒ½é€šè¿‡é—´æ¥æ–¹å¼æ£€æŸ¥
        scheduled_stage = cfg.datasets.vla_data.ecot.scheduled_stage
        if scheduled_stage > 0 and cfg.framework.enable_latent_reasoning:
            # å¯¹äºstage 2+ï¼Œåº”è¯¥ä½¿ç”¨forward_latent
            # å¯ä»¥é€šè¿‡æ£€æŸ¥æ˜¯å¦æœ‰thinking tokensæ¥é—´æ¥éªŒè¯
            check_thinking_token_id = getattr(model.qwen_vl_interface, "thinking_token_id", None)
            if check_thinking_token_id is not None:
                has_thinking_in_batch = (qwen_inputs['input_ids'] == check_thinking_token_id).any().item()
                if has_thinking_in_batch:
                    print(f"âœ… Stage {scheduled_stage}: Thinking tokens found, forward_latent should be used")
                else:
                    print(f"âš ï¸  Stage {scheduled_stage}: No thinking tokens found in batch (unexpected)")
    
    model.train()
    print(f"\nâœ… Forward pass test completed successfully!")
    return output_dict


def test_training_loop(model, dataloader, cfg):
    """
    æµ‹è¯•è®­ç»ƒå¾ªç¯
    
    éªŒè¯:
    1. å¯ä»¥æ­£å¸¸backward
    2. æ¢¯åº¦ä¸ä¸ºNaN/Inf
    3. å‚æ•°å¯ä»¥æ›´æ–°
    4. å¤šæ­¥è®­ç»ƒç¨³å®š
    5. Lossè¶‹åŠ¿åˆç†
    """
    print("\n" + "="*80)
    print("ğŸ‹ï¸  Testing Training Loop")
    print("="*80)
    
    model.train()
    
    # åˆ›å»ºoptimizer
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=cfg.trainer.learning_rate.base,
        betas=tuple(cfg.trainer.optimizer.betas),
        weight_decay=cfg.trainer.optimizer.weight_decay,
        eps=cfg.trainer.optimizer.eps,
    )
    
    # è®°å½•åˆå§‹å‚æ•°ï¼ˆç”¨äºéªŒè¯æ›´æ–°ï¼‰
    initial_param = None
    for name, param in model.named_parameters():
        if param.requires_grad:
            initial_param = (name, param.clone().detach())
            break
    
    if initial_param is None:
        print("âš ï¸  Warning: No trainable parameters found!")
        return []
    
    # è®­ç»ƒå¾ªç¯
    losses = []
    data_iter = iter(dataloader)
    
    print(f"\nğŸš€ Starting training loop ({cfg.trainer.max_train_steps} steps)...")
    for step in tqdm(range(cfg.trainer.max_train_steps), desc="Training"):
        # è·å–batch
        try:
            batch = next(data_iter)
        except StopIteration:
            data_iter = iter(dataloader)
            batch = next(data_iter)
        
        # Forward
        optimizer.zero_grad()
        output_dict = model.forward(batch)
        
        # ä½¿ç”¨total_lossï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œå¦åˆ™ç”¨action_loss
        loss = output_dict.get("total_loss", output_dict["action_loss"])
        
        # Backward
        loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦
        has_nan_grad = False
        has_inf_grad = False
        grad_norms = []
        for name, param in model.named_parameters():
            if param.grad is not None:
                if torch.isnan(param.grad).any():
                    has_nan_grad = True
                    print(f"âš ï¸  NaN gradient in {name}")
                if torch.isinf(param.grad).any():
                    has_inf_grad = True
                    print(f"âš ï¸  Inf gradient in {name}")
                grad_norms.append(param.grad.norm().item())
        
        if has_nan_grad or has_inf_grad:
            print(f"âŒ Step {step}: Gradient check FAILED")
            raise RuntimeError(f"Gradient contains NaN or Inf at step {step}")
        
        # Gradient clipping
        if cfg.trainer.gradient_clipping is not None:
            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.trainer.gradient_clipping)
        
        # Optimizer step
        optimizer.step()
        
        # è®°å½•loss
        losses.append(loss.item())
        
        # æ¯æ­¥æ‰“å°
        if step % cfg.trainer.logging_frequency == 0:
            log_str = f"Step {step}: "
            if "action_loss" in output_dict:
                log_str += f"action_loss={output_dict['action_loss'].item():.4f} "
            if "vlm_loss" in output_dict and output_dict["vlm_loss"] is not None:
                log_str += f"vlm_loss={output_dict['vlm_loss'].item():.4f} "
            log_str += f"total_loss={loss.item():.4f}"
            if grad_norms:
                log_str += f" | grad_norm={np.mean(grad_norms):.4f}"
            print(log_str)
    
    # éªŒè¯å‚æ•°æ›´æ–°
    if initial_param is not None:
        name, initial_value = initial_param
        current_value = dict(model.named_parameters())[name]
        param_changed = not torch.equal(initial_value, current_value)
        if param_changed:
            print(f"\nâœ… Parameters updated (checked: {name})")
            # è®¡ç®—å‚æ•°å˜åŒ–é‡
            param_diff = (current_value - initial_value).abs().max().item()
            print(f"  - Max parameter change: {param_diff:.6f}")
        else:
            print(f"\nâš ï¸  Parameters NOT updated (checked: {name})")
    
    # Lossè¶‹åŠ¿
    print(f"\nğŸ“Š Loss Statistics:")
    print(f"  - Initial loss: {losses[0]:.4f}")
    print(f"  - Final loss: {losses[-1]:.4f}")
    print(f"  - Mean loss: {np.mean(losses):.4f}")
    print(f"  - Std loss: {np.std(losses):.4f}")
    print(f"  - Min loss: {np.min(losses):.4f}")
    print(f"  - Max loss: {np.max(losses):.4f}")
    
    # æ£€æŸ¥lossç¨³å®šæ€§
    if len(losses) > 1:
        loss_std = np.std(losses)
        loss_mean = np.mean(losses)
        cv = loss_std / loss_mean if loss_mean > 0 else float('inf')
        if cv < 1.0:
            print(f"âœ… Loss is stable (CV={cv:.3f} < 1.0)")
        else:
            print(f"âš ï¸  Loss has high variance (CV={cv:.3f} >= 1.0)")
    
    print(f"\nâœ… Training loop test completed successfully!")
    return losses


def main():
    """
    ä¸»æµ‹è¯•æµç¨‹ - å‰5ä¸ªæ­¥éª¤
    """
    # è§£æå‚æ•°
    parser = argparse.ArgumentParser(description="ECoT End-to-End Test Training")
    parser.add_argument("--config_yaml", type=str, default="config/test_ecot_stage0.yaml", help="Path to test config YAML")
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    print("Loading configuration...")
    cfg = OmegaConf.load(args.config_yaml)
    
    print("\n" + "="*80)
    print("ğŸš€ ECoT Implicit Reasoning End-to-End Test")
    print("="*80)
    print(f"Config: {args.config_yaml}")
    print(f"Stage: {cfg.datasets.vla_data.ecot.scheduled_stage}")
    print(f"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
    
    # æµ‹è¯•æµç¨‹
    try:
        # 1. éªŒè¯é…ç½®
        print("\n" + "ğŸ”¹"*40)
        print("Step 1/5: Configuration Validation")
        print("ğŸ”¹"*40)
        validate_config(cfg)
        
        # 2. æµ‹è¯•æ•°æ®åŠ è½½
        print("\n" + "ğŸ”¹"*40)
        print("Step 2/5: DataLoader Test")
        print("ğŸ”¹"*40)
        dataloader = test_dataloader(cfg)
        if dataloader is None:
            raise RuntimeError("DataLoader test failed")
        
        # 3. æµ‹è¯•æ¨¡å‹æ„å»º
        print("\n" + "ğŸ”¹"*40)
        print("Step 3/5: Model Build Test")
        print("ğŸ”¹"*40)
        model = test_model_build(cfg)
        
        # 4. æµ‹è¯•å‰å‘ä¼ æ’­ï¼ˆæ— æ¢¯åº¦ï¼‰
        print("\n" + "ğŸ”¹"*40)
        print("Step 4/5: Forward Pass Test (No Gradient)")
        print("ğŸ”¹"*40)
        output_dict = test_forward_pass(model, dataloader, cfg)
        
        # 5. æµ‹è¯•è®­ç»ƒå¾ªç¯ï¼ˆæœ‰æ¢¯åº¦ï¼‰
        print("\n" + "ğŸ”¹"*40)
        print("Step 5/5: Training Loop Test")
        print("ğŸ”¹"*40)
        losses = test_training_loop(model, dataloader, cfg)
        
        # æœ€ç»ˆæŠ¥å‘Š
        print("\n" + "="*80)
        print("âœ… ALL TESTS PASSED!")
        print("="*80)
        print(f"âœ… Step 1: Configuration validation - OK")
        print(f"âœ… Step 2: DataLoader test - OK")
        print(f"âœ… Step 3: Model build test - OK")
        print(f"âœ… Step 4: Forward pass test - OK")
        print(f"âœ… Step 5: Training loop test - OK")
        print("\nğŸ‰ ECoT pipeline is ready for full training!")
        
    except Exception as e:
        print("\n" + "="*80)
        print("âŒ TEST FAILED")
        print("="*80)
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

