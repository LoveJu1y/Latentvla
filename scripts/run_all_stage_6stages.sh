#!/usr/bin/env bash
# ============================================================================
# ECoT Multi-Stage Training Script (6 Stages)
# ä¾æ¬¡è¿è¡Œ Stage 0 -> Stage 1 -> Stage 2 -> Stage 3 -> Stage 4 -> Stage 5 -> Stage 6
# æ¯ä¸ª stage ä»å‰ä¸€ä¸ª stage çš„æŒ‡å®š step checkpoint ç»§ç»­è®­ç»ƒ
#
# Stage è¯´æ˜ï¼š
#   - Stage 0-4: çº¯æ¨ç†è®­ç»ƒé˜¶æ®µï¼ˆreasoning_onlyï¼‰
#              - é€æ­¥å‡å°‘æ˜¾å¼ CoTï¼Œå¢åŠ éšå¼æ¨ç†ï¼ˆthinking tokensï¼‰
#              - åªè®­ç»ƒ VLM çš„éšå¼æ¨ç†èƒ½åŠ›
#              - Action head å‚æ•°å†»ç»“ï¼ˆrequires_grad=Falseï¼‰
#              - Stage 0-3: batch_size=16
#              - Stage 4: batch_size=32ï¼Œä½¿ç”¨ Stage 4 æ•°æ®é…ç½®
#   - Stage 5: åŠ¨ä½œå¤´ä¸“é¡¹è®­ç»ƒï¼ˆaction_onlyï¼‰
#              - ä½¿ç”¨ Stage 4 çš„æ•°æ®é…ç½®ï¼ˆscheduled_stage=4ï¼‰
#              - VLM å†»ç»“ï¼Œåªè®­ç»ƒ action head
#              - batch_size=32
#   - Stage 6: æœ€ç»ˆè”åˆå¾®è°ƒï¼ˆfullï¼‰
#              - ä½¿ç”¨ Stage 4 çš„æ•°æ®é…ç½®ï¼ˆscheduled_stage=4ï¼‰
#              - VLM + action head ä¸€èµ·å¾®è°ƒ
#              - batch_size=32
# ============================================================================

set -e

# ============================================================================
# ç¯å¢ƒå˜é‡
# ============================================================================
export HF_ENDPOINT="https://hf-mirror.com"
export HF_HOME="/share/project/lvjing/starVLA/qwen_cache"
export WANDB_API_KEY="a8989c35c0573184da807b8a781d72936fe7e379"
export TOKENIZERS_PARALLELISM="false"
export WANDB_BASE_URL="https://api.bandw.top"

# ============================================================================
# é…ç½®å‚æ•° - åœ¨è¿™é‡Œä¿®æ”¹é…ç½®
# ============================================================================

# åŸºç¡€é…ç½®
RUN_ROOT_DIR="./4B_train_vlm_only/outputs"
num_gpus=8
CONFIG_YAML="config/training/ecot_stage2_full.yaml"
WANDB_PROJECT="4B_Latent_qwengr00t_vlm_only1"
# ============================================================================
# è·³è¿‡ Stage é…ç½®ï¼ˆå¯é€‰ï¼‰
# å¦‚æœæƒ³ä»æŸä¸ª Stage å¼€å§‹è®­ç»ƒï¼ˆè·³è¿‡å‰é¢çš„ stagesï¼‰ï¼Œå¯ä»¥è®¾ç½®ï¼š
# START_STAGEï¼šä»å“ªä¸ª stage å¼€å§‹è®­ç»ƒï¼ˆ0-6ï¼‰
# PREV_STAGE_CHECKPOINTï¼šä¸Šä¸€ä¸ª stage çš„ checkpoint è·¯å¾„
# 
# ä¾‹å¦‚ï¼šä» Stage 2 å¼€å§‹è®­ç»ƒï¼Œè·³è¿‡ Stage 0 å’Œ Stage 1
# START_STAGE=2
# PREV_STAGE_CHECKPOINT="/path/to/stage1/checkpoint.pt"
# 
# å¦‚æœ START_STAGE=0ï¼Œåˆ™ä»å¤´å¼€å§‹è®­ç»ƒæ‰€æœ‰ stages
# è¯·è®¾ç½® START_STAGE > 0ï¼Œå¹¶æä¾›ä¸Šä¸€ Stage çš„ checkpoint
# ============================================================================

START_STAGE=3
# å½“ START_STAGE > 0 æ—¶ï¼Œå¿…é¡»æä¾› PREV_STAGE_CHECKPOINTï¼Œç”¨äºåˆå§‹åŒ–åç»­è®­ç»ƒ
PREV_STAGE_CHECKPOINT="/share/project/lvjing/starVLA/4B_train_vlm_only/outputs/ecot_stage2/checkpoints/steps_2500_pytorch_model.pt"
# ç¤ºä¾‹ï¼šä» Stage 2 å¼€å§‹è®­ç»ƒï¼Œä½¿ç”¨ Stage 1 çš„ checkpoint
# START_STAGE=2
# PREV_STAGE_CHECKPOINT="/share/project/lvjing/starVLA/train_6stages/outputs/ecot_stage1/checkpoints/steps_5000_pytorch_model.pt"

# ============================================================================
# æƒé‡é‡è½½é…ç½®ï¼ˆæ¶æ„å˜æ›´æ—¶ä½¿ç”¨ï¼‰
# ============================================================================
# å¦‚æœä»æ—§æ¶æ„ï¼ˆå¦‚ DiT-Bï¼‰è¿ç§»åˆ°æ–°æ¶æ„ï¼ˆå¦‚ DiT-Lï¼‰ï¼Œéœ€è¦å¿½ç•¥å½¢çŠ¶ä¸åŒ¹é…çš„å±‚
# è®¾ç½®ä¸º "qwen_vl_interface" å¯åªåŠ è½½ VLM æƒé‡ï¼›æ­£å¸¸æƒ…å†µè¯·ç•™ç©º ""
RELOAD_MODULES=""

# éªŒè¯é…ç½®
ALL_STAGES=(0 1 2 3 4 5 6)
ALL_STAGES=(0 1 2 3 )
if [ ${START_STAGE} -lt 0 ] || [ ${START_STAGE} -gt 6 ]; then
    echo "âŒ Error: START_STAGE must be between 0 and 6, got: ${START_STAGE}"
    exit 1
fi

if [ ${START_STAGE} -gt 0 ] && [ -z "${PREV_STAGE_CHECKPOINT}" ]; then
    echo "âŒ Error: When START_STAGE > 0, PREV_STAGE_CHECKPOINT must be provided!"
    echo "   You want to start from Stage ${START_STAGE}, but no checkpoint is provided."
    echo "   Please set PREV_STAGE_CHECKPOINT to the checkpoint from Stage $((START_STAGE - 1))."
    exit 1
fi

# æ„å»ºéœ€è¦è®­ç»ƒçš„ Stage åˆ—è¡¨
STAGES=()
for stage in "${ALL_STAGES[@]}"; do
    if [ ${stage} -ge ${START_STAGE} ]; then
        STAGES+=($stage)
    fi
done

if [ ${#STAGES[@]} -eq 0 ]; then
    echo "âŒ Error: No stages to train!"
    exit 1
fi

echo "âœ… Stages to train: ${STAGES[@]}"
if [ ${START_STAGE} -gt 0 ]; then
    echo "â­ï¸  Skipping stages 0-$((START_STAGE - 1))"
    echo "ğŸ“¦ Will use checkpoint: ${PREV_STAGE_CHECKPOINT}"
fi

# Stage é…ç½®ï¼šæ¯ä¸ª stage çš„è®­ç»ƒæ­¥æ•°å’Œä¿å­˜é—´éš”
# Stage 0-4: çº¯æ¨ç†è®­ç»ƒï¼ˆreasoning_onlyï¼Œåªè®­ç»ƒ VLMï¼Œé€æ­¥å¢åŠ éšå¼æ¨ç†ï¼‰
#            Stage 0-3 ä½¿ç”¨ batch_size=16ï¼ŒStage 4 ä½¿ç”¨ batch_size=32
# Stage 5: åŠ¨ä½œå¤´ä¸“é¡¹è®­ç»ƒï¼ˆaction_onlyï¼ŒVLM å†»ç»“ï¼Œåªè®­ç»ƒ action headï¼Œbatch_size=32ï¼‰
# Stage 6: æœ€ç»ˆè”åˆå¾®è°ƒï¼ˆfullï¼ŒVLM + action head ä¸€èµ·å¾®è°ƒï¼Œbatch_size=32ï¼‰
declare -A STAGE_STEPS=(
    [0]=2500   # Stage 0 è®­ç»ƒæ­¥æ•°ï¼ˆçº¯æ¨ç†è®­ç»ƒï¼Œå…¨ CoTï¼‰
    [1]=2500   # Stage 1 è®­ç»ƒæ­¥æ•°ï¼ˆçº¯æ¨ç†è®­ç»ƒï¼‰
    [2]=2500   # Stage 2 è®­ç»ƒæ­¥æ•°ï¼ˆçº¯æ¨ç†è®­ç»ƒï¼‰
    [3]=2500   # Stage 3 è®­ç»ƒæ­¥æ•°ï¼ˆçº¯æ¨ç†è®­ç»ƒï¼‰
    [4]=500   # Stage 4 è®­ç»ƒæ­¥æ•°ï¼ˆå®Œæ•´è®­ç»ƒï¼ŒVLM + action headï¼‰
    [5]=5000   # Stage 5 è®­ç»ƒæ­¥æ•°ï¼ˆaction_onlyï¼Œåªè®­ç»ƒ action headï¼‰
    [6]=60000   # Stage 6 è®­ç»ƒæ­¥æ•°ï¼ˆæœ€ç»ˆè”åˆå¾®è°ƒï¼‰
)

declare -A STAGE_SAVE_INTERVALS=(
    [0]=2500    # Stage 0 ä¿å­˜é—´éš”
    [1]=2500    # Stage 1 ä¿å­˜é—´éš”
    [2]=2500    # Stage 2 ä¿å­˜é—´éš”
    [3]=2500    # Stage 3 ä¿å­˜é—´éš”
    [4]=500    # Stage 4 ä¿å­˜é—´éš”
    [5]=2500    # Stage 5 ä¿å­˜é—´éš”
    [6]=2500    # Stage 6 ä¿å­˜é—´éš”
)

# æ¯ä¸ª stage ä½¿ç”¨çš„ checkpoint stepï¼ˆä»å‰ä¸€ä¸ª stage åŠ è½½ï¼‰
# æ ¼å¼ï¼šstage -> checkpoint_stepï¼ˆä»å‰ä¸€ä¸ª stage çš„å“ªä¸ª step åŠ è½½ï¼‰
declare -A STAGE_CHECKPOINT_STEPS=(
    [0]=""           # Stage 0 ä¸ä½¿ç”¨ checkpointï¼ˆä»å¤´è®­ç»ƒï¼‰
    [1]="2500"      # Stage 1 ä½¿ç”¨ Stage 0 çš„æœ€ç»ˆ checkpoint
    [2]="2500"      # Stage 2 ä½¿ç”¨ Stage 1 çš„æœ€ç»ˆ checkpoint
    [3]="2500"      # Stage 3 ä½¿ç”¨ Stage 2 çš„æœ€ç»ˆ checkpoint
    [4]="2500"      # Stage 4 ä½¿ç”¨ Stage 3 çš„æœ€ç»ˆ checkpoint
    [5]="500"      # Stage 5 ä½¿ç”¨ Stage 4 çš„æœ€ç»ˆ checkpoint
    [6]="60000"      # Stage 6 ä½¿ç”¨ Stage 5 çš„æœ€ç»ˆ checkpoint
)

# ============================================================================
# è®­ç»ƒå‡½æ•°
# ============================================================================

run_stage() {
    local stage=$1
    local prev_checkpoint=$2  # ä¸Šä¸€ä¸ª stage çš„ checkpoint è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    local reload_modules=$3   # [New] ä»…é‡è½½æŒ‡å®šæ¨¡å—ï¼ˆç”¨äºæ¶æ„å˜æ›´æ—¶çš„éƒ¨åˆ†åŠ è½½ï¼‰
    
    echo ""
    echo "============================================================================"
    echo "ğŸš€ Starting Stage ${stage} Training"
    echo "============================================================================"
    
    # åˆ›å»º stage ç‰¹å®šçš„ run IDï¼ˆä¸ä½¿ç”¨æ—¥æœŸï¼Œæ›´ç®€å•ç›´è§‚ï¼‰
    STAGE_RUN_ID="ecot_stage${stage}"
    STAGE_OUTPUT_DIR="${RUN_ROOT_DIR}/${STAGE_RUN_ID}"
    mkdir -p ${STAGE_OUTPUT_DIR}
    
    # æ ¹æ® stage å†³å®šè®­ç»ƒé˜¶æ®µæ¨¡å¼å’Œæ•°æ®é…ç½®
    # Stage 0-4: çº¯æ¨ç†è®­ç»ƒï¼ˆåªè®­ç»ƒ VLMï¼Œå†»ç»“ action headï¼‰
    # Stage 5: action_onlyï¼ˆVLM å†»ç»“ï¼Œåªè®­ç»ƒ action headï¼Œä½¿ç”¨ stage 4 çš„æ•°æ®ï¼‰
    # Stage 6: æœ€ç»ˆè”åˆå¾®è°ƒï¼ˆVLM + action headï¼Œä½¿ç”¨ stage 4 çš„æ•°æ®ï¼‰
    if [ ${stage} -lt 5 ]; then
        TRAINING_STAGE="reasoning_only"
        VLM_LOSS_WEIGHT=1.0  # Stage 0-3 çº¯æ¨ç†ï¼Œæƒé‡è®¾ä¸º 1.0
        DATA_STAGE=${stage}   # æ•°æ®é…ç½®è·Ÿéšå®é™… stage
        echo "ğŸ§  [Stage ${stage}] Reasoning-only mode: Training VLM reasoning, action_model frozen"
    elif [ ${stage} -eq 5 ]; then
        TRAINING_STAGE="action_only"
        VLM_LOSS_WEIGHT=0.0  # action_only æ¨¡å¼ä¸éœ€è¦ vlm_loss
        DATA_STAGE=4  # ä½¿ç”¨ Stage 4 çš„æ•°æ®é…ç½®
        echo "ğŸ”§ [Stage ${stage}] Action-only mode: VLM frozen, training action_model only (using Stage 4 data)"
    else
        TRAINING_STAGE="full"
        VLM_LOSS_WEIGHT=0.1  # Stage 6 æœ€ç»ˆå¾®è°ƒ
        DATA_STAGE=4  # ä½¿ç”¨ Stage 4 çš„æ•°æ®é…ç½®
        echo "ğŸ¯ [Stage ${stage}] Final fine-tuning: VLM + action_model both trainable (using Stage 4 data)"
    fi
    
    # æ ¹æ® stage å†³å®š batch size
    # Stage 0-3: batch_size=12
    # Stage 4-6: batch_size=16
    if [ ${stage} -ge 3 ]; then
        BATCH_SIZE=16
    else
        BATCH_SIZE=12
    fi
    echo "ğŸ“Š Using batch size: ${BATCH_SIZE} (Stage ${stage})"
    
    # æ„å»ºè®­ç»ƒå‚æ•°ï¼ˆä¸ run_ecot_8gpu.sh ä¿æŒä¸€è‡´ï¼‰
    TRAIN_CONFIG_ARGS=(
        --trainer.max_train_steps ${STAGE_STEPS[$stage]}
        --trainer.save_interval ${STAGE_SAVE_INTERVALS[$stage]}
        --trainer.logging_frequency 10
        --trainer.eval_interval 60000000
        --trainer.learning_rate.base 3.0e-5
        --trainer.learning_rate.action_model 5.0e-5
        --trainer.gradient_accumulation_steps 1
        --framework.qwenvl.model_max_length 2048
        --framework.action_model.action_model_type DiT-L
        --framework.action_model.diffusion_model_cfg.num_layers 16
        --framework.training_stage ${TRAINING_STAGE}
        --framework.latent_reasoning.vlm_loss_weight ${VLM_LOSS_WEIGHT}
        --datasets.vla_data.per_device_batch_size ${BATCH_SIZE}
        --datasets.vla_data.ecot.scheduled_stage ${DATA_STAGE}
        --datasets.vla_data.num_workers 0
    )
    
    # å¦‚æœæä¾›äº†ä¸Šä¸€ä¸ª stage çš„ checkpointï¼Œæ·»åŠ é¢„è®­ç»ƒ checkpoint å‚æ•°
    if [ -n "${prev_checkpoint}" ] && [ -f "${prev_checkpoint}" ]; then
        TRAIN_CONFIG_ARGS+=(
            --trainer.pretrained_checkpoint "${prev_checkpoint}"
        )
        # å¦‚æœæŒ‡å®šäº†éƒ¨åˆ†åŠ è½½ï¼ˆä¾‹å¦‚åªåŠ è½½ VLMï¼‰ï¼Œæ·»åŠ å‚æ•°
        if [ -n "${reload_modules}" ]; then
            TRAIN_CONFIG_ARGS+=( --trainer.reload_modules "${reload_modules}" )
            echo "âš ï¸  Partial load enabled: Only reloading modules: ${reload_modules}"
        fi
        echo "ğŸ“¦ Loading checkpoint from previous stage: ${prev_checkpoint}"
    else
        echo "ğŸ†• Starting from scratch (no previous checkpoint)"
    fi
    
    # W&B å’Œæ•°æ®é…ç½®ï¼ˆä¸ run_ecot_8gpu.sh ä¿æŒä¸€è‡´ï¼‰
    BASE_CONFIG_ARGS=(
        --wandb_project ${WANDB_PROJECT}
        --wandb_entity "lvj2114-beijing-academy-of-artificial-intelligence"
        --datasets.vla_data.ecot.data_root_dir "/share/project/emllm_mnt.1d/mnt/sfs/baaiei/jyShi/rt_newData"
        --datasets.vla_data.ecot.data_mix "bridge"
    )
    
    # ä¿å­˜è„šæœ¬å‰¯æœ¬
    cp $0 ${STAGE_OUTPUT_DIR}/
    
    # å¯åŠ¨è®­ç»ƒ
    accelerate launch \
        --config_file starVLA/config/deepseeds/deepspeed_zero2.yaml \
        --num_processes ${num_gpus} \
        starVLA/training/train_ecot.py \
        --config_yaml ${CONFIG_YAML} \
        --run_root_dir ${RUN_ROOT_DIR} \
        --run_id ${STAGE_RUN_ID} \
        "${TRAIN_CONFIG_ARGS[@]}" \
        "${BASE_CONFIG_ARGS[@]}"
    
    # æ£€æŸ¥è®­ç»ƒæ˜¯å¦æˆåŠŸ
    TRAIN_EXIT_CODE=$?
    if [ ${TRAIN_EXIT_CODE} -ne 0 ]; then
        echo "âŒ Stage ${stage} training failed!" >&2
        return ${TRAIN_EXIT_CODE}
    fi
    
    echo ""
    echo "âœ… Stage ${stage} training completed successfully!"
    echo "ğŸ“ Output directory: ${STAGE_OUTPUT_DIR}"
}

# ============================================================================
# ä¸»è®­ç»ƒæµç¨‹
# ============================================================================

echo "============================================================================"
echo "ğŸ¯ ECoT 6-Stage Training Pipeline"
echo "============================================================================"
echo "Stages to run: ${STAGES[@]}"
if [ ${START_STAGE} -gt 0 ]; then
    echo "  - Skipped stages: 0-$((START_STAGE - 1)) (using checkpoint from Stage $((START_STAGE - 1)))"
fi
echo "  - Stage 0-4: çº¯æ¨ç†è®­ç»ƒï¼ˆreasoning_onlyï¼Œåªè®­ç»ƒ VLMï¼Œaction head å†»ç»“ï¼‰"
echo "               Stage 0-3: batch_size=16ï¼ŒStage 4: batch_size=32"
echo "  - Stage 5: åŠ¨ä½œå¤´ä¸“é¡¹è®­ç»ƒï¼ˆaction_onlyï¼ŒVLM å†»ç»“ï¼Œbatch_size=32ï¼‰"
echo "  - Stage 6: æœ€ç»ˆè”åˆå¾®è°ƒï¼ˆfullï¼ŒVLM + action headï¼Œbatch_size=32ï¼‰"
echo "Number of GPUs: ${num_gpus}"
echo "Config: ${CONFIG_YAML}"
echo "Output directories: ${RUN_ROOT_DIR}/ecot_stage{${STAGES[@]}}"
echo "============================================================================"

# è®°å½•å¼€å§‹æ—¶é—´
START_TIME=$(date +%s)

# ä¾æ¬¡è¿è¡Œå„ä¸ª stage
PREV_STAGE_OUTPUT_DIR=""
for i in "${!STAGES[@]}"; do
    stage=${STAGES[$i]}
    STAGE_START_TIME=$(date +%s)
    
    # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºå½“å‰æ­£åœ¨å¤„ç†çš„ stage
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "ğŸ“ [Main Loop] Processing Stage ${stage} (index ${i})"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    
    # æ„å»º checkpoint è·¯å¾„ï¼ˆå¦‚æœæœ‰ä¸Šä¸€ä¸ª stageï¼‰
    PREV_CHECKPOINT=""
    
    # æƒ…å†µ1ï¼šå¦‚æœæ˜¯ç¬¬ä¸€ä¸ªè¦è®­ç»ƒçš„ stageï¼Œä¸” START_STAGE > 0ï¼Œä½¿ç”¨æä¾›çš„ checkpoint
    if [ ${i} -eq 0 ] && [ ${START_STAGE} -gt 0 ] && [ -n "${PREV_STAGE_CHECKPOINT}" ]; then
        PREV_CHECKPOINT="${PREV_STAGE_CHECKPOINT}"
        echo "ğŸ” Using provided checkpoint from Stage $((START_STAGE - 1)): ${PREV_CHECKPOINT}"
        
        # æ£€æŸ¥ checkpoint æ˜¯å¦å­˜åœ¨
        if [ ! -f "${PREV_CHECKPOINT}" ]; then
            echo "âŒ Checkpoint not found: ${PREV_CHECKPOINT}"
            echo "   Please check the checkpoint path."
            exit 1
        fi
        echo "âœ… Checkpoint found"
    # æƒ…å†µ2ï¼šæ­£å¸¸æƒ…å†µï¼Œä»å‰ä¸€ä¸ª stage çš„è¾“å‡ºç›®å½•åŠ è½½ checkpoint
    elif [ -n "${PREV_STAGE_OUTPUT_DIR}" ] && [ -n "${STAGE_CHECKPOINT_STEPS[$stage]}" ]; then
        checkpoint_step=${STAGE_CHECKPOINT_STEPS[$stage]}
        PREV_CHECKPOINT="${PREV_STAGE_OUTPUT_DIR}/checkpoints/steps_${checkpoint_step}_pytorch_model.pt"
        
        echo "ğŸ” Checking for checkpoint: ${PREV_CHECKPOINT}"
        
        # æ£€æŸ¥ checkpoint æ˜¯å¦å­˜åœ¨
        if [ ! -f "${PREV_CHECKPOINT}" ]; then
            echo "âŒ Checkpoint not found: ${PREV_CHECKPOINT}"
            echo "   Please check if the previous stage completed successfully."
            exit 1
        fi
        echo "âœ… Checkpoint found"
    else
        echo "â„¹ï¸  No previous checkpoint (starting from scratch)"
    fi
    
    echo "ğŸš€ Now calling run_stage function..."
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦ reload_modules (ä»…åœ¨ç¬¬ä¸€ä¸ªè¿è¡Œçš„ stage ä¸”æœ‰å¤–éƒ¨ checkpoint æ—¶ä½¿ç”¨)
    RELOAD_OPT=""
    if [ "${stage}" == "${STAGES[0]}" ] && [ "${START_STAGE}" -gt 0 ]; then
        RELOAD_OPT="${RELOAD_MODULES}"
    fi

    # ç›´æ¥è¿è¡Œ run_stageï¼Œå®æ—¶æ˜¾ç¤ºè¾“å‡ºï¼ˆä¸æ•è·ï¼‰
    run_stage ${stage} "${PREV_CHECKPOINT}" "${RELOAD_OPT}"
    EXIT_CODE=$?
    
    # æ£€æŸ¥è®­ç»ƒæ˜¯å¦æˆåŠŸ
    if [ ${EXIT_CODE} -ne 0 ]; then
        echo "âŒ Stage ${stage} training failed with exit code ${EXIT_CODE}!"
        exit 1
    fi
    
    # æ„å»ºå½“å‰ stage çš„è¾“å‡ºç›®å½•ï¼ˆå·²çŸ¥è·¯å¾„ï¼‰
    CURRENT_STAGE_OUTPUT_DIR="${RUN_ROOT_DIR}/ecot_stage${stage}"
    
    # éªŒè¯è¾“å‡ºç›®å½•æ˜¯å¦å­˜åœ¨
    if [ ! -d "${CURRENT_STAGE_OUTPUT_DIR}" ]; then
        echo "âŒ Stage ${stage} output directory not found: ${CURRENT_STAGE_OUTPUT_DIR}"
        exit 1
    fi
    
    # æ›´æ–°ä¸Šä¸€ä¸ª stage çš„è¾“å‡ºç›®å½•
    PREV_STAGE_OUTPUT_DIR="${CURRENT_STAGE_OUTPUT_DIR}"
    
    # è®¡ç®— stage è€—æ—¶
    STAGE_END_TIME=$(date +%s)
    STAGE_DURATION=$((STAGE_END_TIME - STAGE_START_TIME))
    STAGE_HOURS=$((STAGE_DURATION / 3600))
    STAGE_MINS=$(((STAGE_DURATION % 3600) / 60))
    
    echo ""
    echo "â±ï¸  Stage ${stage} took ${STAGE_HOURS}h ${STAGE_MINS}m"
    echo "============================================================================"
done

# è®¡ç®—æ€»è€—æ—¶
END_TIME=$(date +%s)
TOTAL_DURATION=$((END_TIME - START_TIME))
TOTAL_HOURS=$((TOTAL_DURATION / 3600))
TOTAL_MINS=$(((TOTAL_DURATION % 3600) / 60))

# æ„å»ºæœ€ç»ˆ checkpoint è·¯å¾„ï¼ˆä½¿ç”¨æœ€åä¸€ä¸ª stage çš„æœ€ç»ˆè®­ç»ƒæ­¥æ•°ï¼‰
FINAL_STAGE=${STAGES[-1]}
FINAL_CHECKPOINT_STEP=${STAGE_STEPS[$FINAL_STAGE]}
FINAL_CHECKPOINT="${PREV_STAGE_OUTPUT_DIR}/checkpoints/steps_${FINAL_CHECKPOINT_STEP}_pytorch_model.pt"

echo ""
echo "============================================================================"
echo "âœ… All 6 stages completed successfully!"
echo "============================================================================"
echo "Total training time: ${TOTAL_HOURS}h ${TOTAL_MINS}m"
echo "Final checkpoint: ${FINAL_CHECKPOINT}"
echo ""
echo "ğŸ“‹ Training Summary:"
echo "  Stage 0-4: VLM reasoning training (reasoning_only)"
echo "             Stage 0-3: batch_size=16, Stage 4: batch_size=32"
echo "  Stage 5:   Action head specialization (action_only, batch_size=32)"
echo "  Stage 6:   Final joint fine-tuning (full, batch_size=32)"
echo "============================================================================"

