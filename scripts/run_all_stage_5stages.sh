#!/usr/bin/env bash
# ============================================================================
# ECoT Multi-Stage Training Script
# ä¾æ¬¡è¿è¡Œ Stage 0 -> Stage 1 -> Stage 2 -> Stage 3 -> Stage 4
# æ¯ä¸ª stage ä»å‰ä¸€ä¸ª stage çš„æŒ‡å®š step checkpoint ç»§ç»­è®­ç»ƒ
#
# Stage è¯´æ˜ï¼š
#   - Stage 0-3: çº¯æ¨ç†è®­ç»ƒé˜¶æ®µï¼ˆreasoning_onlyï¼‰
#              - é€æ­¥å‡å°‘æ˜¾å¼ CoTï¼Œå¢åŠ éšå¼æ¨ç†ï¼ˆthinking tokensï¼‰
#              - åªè®­ç»ƒ VLM çš„éšå¼æ¨ç†èƒ½åŠ›
#              - Action head å‚æ•°å†»ç»“ï¼ˆrequires_grad=Falseï¼‰
#   - Stage 4: å®Œæ•´è®­ç»ƒé˜¶æ®µï¼ˆfullï¼‰
#              - æœ€å°‘æˆ–æ²¡æœ‰æ˜¾å¼ CoTï¼Œä¸»è¦éšå¼æ¨ç†
#              - VLM + action head ä¸€èµ·ç«¯åˆ°ç«¯è®­ç»ƒ
# ============================================================================

set -e

# ============================================================================
# ç¯å¢ƒå˜é‡
# ============================================================================
export HF_ENDPOINT="https://hf-mirror.com"
export HF_HOME="/share/project/lvjing/starVLA/qwen_cache"
export WANDB_API_KEY="a8989c35c0573184da807b8a781d72936fe7e379"
export TOKENIZERS_PARALLELISM="false"
export WANDB_BASE_URL="https://api.bandw.top"

# ============================================================================
# é…ç½®å‚æ•° - åœ¨è¿™é‡Œä¿®æ”¹é…ç½®
# ============================================================================

# åŸºç¡€é…ç½®
RUN_ROOT_DIR="./4B_train_5stages/outputs_3"
num_gpus=8
CONFIG_YAML="config/training/ecot_stage2_full.yaml"
WANDB_PROJECT="4B_Latent_qwengr00t_5stage_1"
# ============================================================================
# è·³è¿‡å‰è‹¥å¹² Stage é…ç½®ï¼ˆå¯é€‰ï¼‰
# å¦‚æœå¸Œæœ›ä»æŸä¸ª stage ä¹‹åå¼€å§‹è®­ç»ƒï¼ˆä¾‹å¦‚è·³è¿‡ Stage 0ã€1ï¼Œä» Stage 2 å¼€å§‹ï¼‰ï¼Œ
# è¯·è®¾ç½® START_STAGE > 0ï¼Œå¹¶æä¾›ä¸Šä¸€ Stage çš„ checkpoint
# ============================================================================
START_STAGE=4
# å½“ START_STAGE > 0 æ—¶ï¼Œå¿…é¡»æä¾› PREV_STAGE_CHECKPOINTï¼Œç”¨äºåˆå§‹åŒ–åç»­è®­ç»ƒ
PREV_STAGE_CHECKPOINT="/share/project/lvjing/starVLA/4B_train_vlm_only/outputs/ecot_stage3/checkpoints/steps_2500_pytorch_model.pt"
# ç¤ºä¾‹ï¼šä» Stage 2 å¼€å§‹è®­ç»ƒï¼Œä½¿ç”¨ Stage 1 çš„ checkpoint
# START_STAGE=2
# PREV_STAGE_CHECKPOINT="/share/project/lvjing/starVLA/train_2shcedule/outputs1/ecot_stage1/checkpoints/steps_5000_pytorch_model.pt"

# ============================================================================
# æƒé‡é‡è½½é…ç½®ï¼ˆæ¶æ„å˜æ›´æ—¶ä½¿ç”¨ï¼‰
# ============================================================================
# å¦‚æœä»æ—§æ¶æ„ï¼ˆå¦‚ DiT-Bï¼‰è¿ç§»åˆ°æ–°æ¶æ„ï¼ˆå¦‚ DiT-Lï¼‰ï¼Œéœ€è¦å¿½ç•¥å½¢çŠ¶ä¸åŒ¹é…çš„å±‚
# è®¾ç½®ä¸º "qwen_vl_interface" å¯åªåŠ è½½ VLM æƒé‡ï¼›æ­£å¸¸æƒ…å†µè¯·ç•™ç©º ""
# RELOAD_MODULES="qwen_vl_interface"
RELOAD_MODULES=""
ALL_STAGES=(0 1 2 3 4)
STAGES=()
for stage in "${ALL_STAGES[@]}"; do
    if [ ${stage} -ge ${START_STAGE} ]; then
        STAGES+=($stage)
    fi
done

if [ ${#STAGES[@]} -eq 0 ]; then
    echo "âŒ Error: START_STAGE (${START_STAGE}) exceeds available stages!"
    exit 1
fi

if [ ${START_STAGE} -gt 0 ] && [ -z "${PREV_STAGE_CHECKPOINT}" ]; then
    echo "âŒ Error: START_STAGE=${START_STAGE} but PREV_STAGE_CHECKPOINT is empty."
    echo "   Please provide the checkpoint of Stage $((START_STAGE-1))."
    exit 1
fi

# Stage é…ç½®ï¼šæ¯ä¸ª stage çš„è®­ç»ƒæ­¥æ•°å’Œä¿å­˜é—´éš”
# Stage 0-3: çº¯æ¨ç†è®­ç»ƒï¼ˆreasoning_onlyï¼Œåªè®­ç»ƒ VLMï¼Œé€æ­¥å¢åŠ éšå¼æ¨ç†ï¼‰
# Stage 4: å®Œæ•´è®­ç»ƒï¼ˆfullï¼ŒVLM + action head ç«¯åˆ°ç«¯è®­ç»ƒï¼‰
declare -A STAGE_STEPS=(
    [0]=10000   # Stage 0 è®­ç»ƒæ­¥æ•°ï¼ˆçº¯æ¨ç†è®­ç»ƒï¼Œå…¨ CoTï¼‰
    [1]=8000   # Stage 1 è®­ç»ƒæ­¥æ•°ï¼ˆçº¯æ¨ç†è®­ç»ƒï¼‰
    [2]=8000  # Stage 2 è®­ç»ƒæ­¥æ•°ï¼ˆçº¯æ¨ç†è®­ç»ƒï¼‰
    [3]=8000   # Stage 3 è®­ç»ƒæ­¥æ•°ï¼ˆçº¯æ¨ç†è®­ç»ƒï¼‰
    [4]=60000   # Stage 4 è®­ç»ƒæ­¥æ•°ï¼ˆå®Œæ•´è®­ç»ƒï¼ŒVLM + action headï¼‰
)

declare -A STAGE_SAVE_INTERVALS=(
    [0]=5000    # Stage 0 ä¿å­˜é—´éš”
    [1]=4000    # Stage 1 ä¿å­˜é—´éš”
    [2]=4000   # Stage 2 ä¿å­˜é—´éš”
    [3]=4000    # Stage 3 ä¿å­˜é—´éš”
    [4]=2500    # Stage 4 ä¿å­˜é—´éš”
)

# æ¯ä¸ª stage ä½¿ç”¨çš„ checkpoint stepï¼ˆä»å‰ä¸€ä¸ª stage åŠ è½½ï¼‰
# æ ¼å¼ï¼šstage -> checkpoint_stepï¼ˆä»å‰ä¸€ä¸ª stage çš„å“ªä¸ª step åŠ è½½ï¼‰
declare -A STAGE_CHECKPOINT_STEPS=(
    [0]=""           # Stage 0 ä¸ä½¿ç”¨ checkpointï¼ˆä»å¤´è®­ç»ƒï¼‰
    [1]="10000"      # Stage 1 ä½¿ç”¨ Stage 0 çš„ steps_50000 checkpoint
    [2]="8000"      # Stage 2 ä½¿ç”¨ Stage 1 çš„ steps_10000 checkpoint
    [3]="8000"      # Stage 3 ä½¿ç”¨ Stage 2 çš„ steps_10000 checkpoint
    [4]="8000"      # Stage 4 ä½¿ç”¨ Stage 3 çš„ steps_10000 checkpoint
)

# ============================================================================
# è®­ç»ƒå‡½æ•°
# ============================================================================

run_stage() {
    local stage=$1
    local prev_checkpoint=$2  # ä¸Šä¸€ä¸ª stage çš„ checkpoint è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    local reload_modules=$3   # [New] ä»…é‡è½½æŒ‡å®šæ¨¡å—ï¼ˆç”¨äºæ¶æ„å˜æ›´æ—¶çš„éƒ¨åˆ†åŠ è½½ï¼‰
    
    echo ""
    echo "============================================================================"
    echo "ğŸš€ Starting Stage ${stage} Training"
    echo "============================================================================"
    
    # åˆ›å»º stage ç‰¹å®šçš„ run IDï¼ˆä¸ä½¿ç”¨æ—¥æœŸï¼Œæ›´ç®€å•ç›´è§‚ï¼‰
    STAGE_RUN_ID="ecot_stage${stage}"
    STAGE_OUTPUT_DIR="${RUN_ROOT_DIR}/${STAGE_RUN_ID}"
    mkdir -p ${STAGE_OUTPUT_DIR}
    
    # æ ¹æ® stage å†³å®šè®­ç»ƒé˜¶æ®µæ¨¡å¼
    # Stage 0-3: çº¯æ¨ç†è®­ç»ƒï¼ˆåªè®­ç»ƒ VLMï¼Œå†»ç»“ action headï¼‰
    # Stage 4: å®Œæ•´è®­ç»ƒï¼ˆVLM + action head ä¸€èµ·è®­ç»ƒï¼‰
    if [ ${stage} -lt 4 ]; then
        TRAINING_STAGE="reasoning_only"
        VLM_LOSS_WEIGHT=1.0  # Stage 0-3 çº¯æ¨ç†ï¼Œæƒé‡è®¾ä¸º 1.0
        echo "ğŸ§  [Stage ${stage}] Reasoning-only mode: Training VLM reasoning, action_model frozen"
    else
        TRAINING_STAGE="full"
        VLM_LOSS_WEIGHT=0.1  # Stage 4 å®Œæ•´è®­ç»ƒï¼Œå¹³è¡¡ action_loss å’Œ vlm_loss
        echo "ğŸ¯ [Stage ${stage}] Full training mode: VLM + action_model both trainable"
    fi
    
    # æ ¹æ® stage è®¾ç½® batch size
    # Stage 0-3: batch_size=12
    # Stage 4: batch_size=16
    if [ ${stage} -ge 3 ]; then
        BATCH_SIZE=16
    else
        BATCH_SIZE=12
    fi
    
    # æ„å»ºè®­ç»ƒå‚æ•°ï¼ˆä¸ run_ecot_8gpu.sh ä¿æŒä¸€è‡´ï¼‰
    TRAIN_CONFIG_ARGS=(
        --trainer.max_train_steps ${STAGE_STEPS[$stage]}
        --trainer.save_interval ${STAGE_SAVE_INTERVALS[$stage]}
        --trainer.logging_frequency 10
        --trainer.eval_interval 50000000
        --trainer.learning_rate.base 3.0e-5
        --trainer.gradient_accumulation_steps 1
        --framework.qwenvl.model_max_length 2048
        --framework.action_model.action_model_type DiT-L
        --framework.action_model.diffusion_model_cfg.num_layers 16
        --framework.training_stage ${TRAINING_STAGE}
        --framework.latent_reasoning.vlm_loss_weight ${VLM_LOSS_WEIGHT}
        --datasets.vla_data.per_device_batch_size ${BATCH_SIZE}
        --datasets.vla_data.ecot.scheduled_stage ${stage}
        --datasets.vla_data.num_workers 0
    )
    
    # å¦‚æœæä¾›äº†ä¸Šä¸€ä¸ª stage çš„ checkpointï¼Œæ·»åŠ é¢„è®­ç»ƒ checkpoint å‚æ•°
    if [ -n "${prev_checkpoint}" ] && [ -f "${prev_checkpoint}" ]; then
        TRAIN_CONFIG_ARGS+=(
            --trainer.pretrained_checkpoint "${prev_checkpoint}"
        )
        # å¦‚æœæŒ‡å®šäº†éƒ¨åˆ†åŠ è½½ï¼ˆä¾‹å¦‚åªåŠ è½½ VLMï¼‰ï¼Œæ·»åŠ å‚æ•°
        if [ -n "${reload_modules}" ]; then
            TRAIN_CONFIG_ARGS+=( --trainer.reload_modules "${reload_modules}" )
            echo "âš ï¸  Partial load enabled: Only reloading modules: ${reload_modules}"
        fi
        echo "ğŸ“¦ Loading checkpoint from previous stage: ${prev_checkpoint}"
    else
        echo "ğŸ†• Starting from scratch (no previous checkpoint)"
    fi
    
    # W&B å’Œæ•°æ®é…ç½®ï¼ˆä¸ run_ecot_8gpu.sh ä¿æŒä¸€è‡´ï¼‰
    BASE_CONFIG_ARGS=(
        --wandb_project "${WANDB_PROJECT}"
        --wandb_entity "lvj2114-beijing-academy-of-artificial-intelligence"
        --datasets.vla_data.ecot.data_root_dir "/share/project/emllm_mnt.1d/mnt/sfs/baaiei/jyShi/rt_newData"
        --datasets.vla_data.ecot.data_mix "bridge"
    )
    
    # ä¿å­˜è„šæœ¬å‰¯æœ¬
    cp $0 ${STAGE_OUTPUT_DIR}/
    
    # å¯åŠ¨è®­ç»ƒ
    accelerate launch \
        --config_file starVLA/config/deepseeds/deepspeed_zero2.yaml \
        --num_processes ${num_gpus} \
        starVLA/training/train_ecot.py \
        --config_yaml ${CONFIG_YAML} \
        --run_root_dir ${RUN_ROOT_DIR} \
        --run_id ${STAGE_RUN_ID} \
        "${TRAIN_CONFIG_ARGS[@]}" \
        "${BASE_CONFIG_ARGS[@]}"
    
    # æ£€æŸ¥è®­ç»ƒæ˜¯å¦æˆåŠŸ
    TRAIN_EXIT_CODE=$?
    if [ ${TRAIN_EXIT_CODE} -ne 0 ]; then
        echo "âŒ Stage ${stage} training failed!" >&2
        return ${TRAIN_EXIT_CODE}
    fi
    
    echo ""
    echo "âœ… Stage ${stage} training completed successfully!"
    echo "ğŸ“ Output directory: ${STAGE_OUTPUT_DIR}"
}

# ============================================================================
# ä¸»è®­ç»ƒæµç¨‹
# ============================================================================

echo "============================================================================"
echo "ğŸ¯ ECoT Multi-Stage Training Pipeline"
echo "============================================================================"
echo "Stages to run: ${STAGES[@]}"
if [ ${START_STAGE} -gt 0 ]; then
    echo "  - Stages 0~$((START_STAGE-1)): è·³è¿‡ï¼ˆä½¿ç”¨æä¾›çš„ checkpoint: ${PREV_STAGE_CHECKPOINT}ï¼‰"
    echo "  - Stage ${START_STAGE} èµ·ï¼šæŒ‰é…ç½®è®­ç»ƒ"
else
    echo "  - Stage 0-3: çº¯æ¨ç†è®­ç»ƒï¼ˆreasoning_onlyï¼Œåªè®­ç»ƒ VLMï¼Œaction head å†»ç»“ï¼‰"
    echo "  - Stage 4: å®Œæ•´è®­ç»ƒï¼ˆfullï¼ŒVLM + action head ç«¯åˆ°ç«¯è®­ç»ƒï¼‰"
fi
echo "Number of GPUs: ${num_gpus}"
echo "Config: ${CONFIG_YAML}"
echo "Output directories: ${RUN_ROOT_DIR}/ecot_stage{${STAGES[@]}}"
echo "============================================================================"

# è®°å½•å¼€å§‹æ—¶é—´
START_TIME=$(date +%s)

# ä¾æ¬¡è¿è¡Œå„ä¸ª stage
PREV_STAGE_OUTPUT_DIR=""
FIRST_STAGE=${STAGES[0]}
for i in "${!STAGES[@]}"; do
    stage=${STAGES[$i]}
    STAGE_START_TIME=$(date +%s)
    
    # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºå½“å‰æ­£åœ¨å¤„ç†çš„ stage
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "ğŸ“ [Main Loop] Processing Stage ${stage} (index ${i})"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    
    # æ„å»º checkpoint è·¯å¾„ï¼ˆå¦‚æœæœ‰ä¸Šä¸€ä¸ª stageï¼‰
    PREV_CHECKPOINT=""
    
    if [ ${stage} -eq ${FIRST_STAGE} ]; then
        if [ ${stage} -eq 0 ]; then
            echo "â„¹ï¸  Starting Stage 0 from scratch (no previous checkpoint)"
        else
            PREV_CHECKPOINT="${PREV_STAGE_CHECKPOINT}"
            echo "ğŸ” Using provided checkpoint from Stage $((stage-1)): ${PREV_CHECKPOINT}"
            if [ ! -f "${PREV_CHECKPOINT}" ]; then
                echo "âŒ Provided checkpoint not found: ${PREV_CHECKPOINT}"
                exit 1
            fi
            echo "âœ… Checkpoint found"
        fi
    else
        if [ -n "${PREV_STAGE_OUTPUT_DIR}" ] && [ -n "${STAGE_CHECKPOINT_STEPS[$stage]}" ]; then
            checkpoint_step=${STAGE_CHECKPOINT_STEPS[$stage]}
            PREV_CHECKPOINT="${PREV_STAGE_OUTPUT_DIR}/checkpoints/steps_${checkpoint_step}_pytorch_model.pt"
            echo "ğŸ” Checking for checkpoint from Stage $((stage-1)): ${PREV_CHECKPOINT}"
            if [ ! -f "${PREV_CHECKPOINT}" ]; then
                echo "âŒ Checkpoint not found: ${PREV_CHECKPOINT}"
                echo "   Please check if Stage $((stage-1)) completed successfully."
                exit 1
            fi
            echo "âœ… Checkpoint found"
        else
            echo "â„¹ï¸  No previous checkpoint available; starting from scratch"
        fi
    fi
    
    echo "ğŸš€ Now calling run_stage function..."
    # ç›´æ¥è¿è¡Œ run_stageï¼Œå®æ—¶æ˜¾ç¤ºè¾“å‡ºï¼ˆä¸æ•è·ï¼‰
    # åˆ¤æ–­æ˜¯å¦éœ€è¦ reload_modules (ä»…åœ¨ç¬¬ä¸€ä¸ªè¿è¡Œçš„ stage ä¸”æœ‰å¤–éƒ¨ checkpoint æ—¶ä½¿ç”¨)
    RELOAD_OPT=""
    if [ "${stage}" == "${STAGES[0]}" ] && [ "${START_STAGE}" -gt 0 ]; then
        RELOAD_OPT="${RELOAD_MODULES}"
    fi
    
    run_stage ${stage} "${PREV_CHECKPOINT}" "${RELOAD_OPT}"
    EXIT_CODE=$?
    
    # æ£€æŸ¥è®­ç»ƒæ˜¯å¦æˆåŠŸ
    if [ ${EXIT_CODE} -ne 0 ]; then
        echo "âŒ Stage ${stage} training failed with exit code ${EXIT_CODE}!"
        exit 1
    fi
    
    # æ„å»ºå½“å‰ stage çš„è¾“å‡ºç›®å½•ï¼ˆå·²çŸ¥è·¯å¾„ï¼‰
    CURRENT_STAGE_OUTPUT_DIR="${RUN_ROOT_DIR}/ecot_stage${stage}"
    
    # éªŒè¯è¾“å‡ºç›®å½•æ˜¯å¦å­˜åœ¨
    if [ ! -d "${CURRENT_STAGE_OUTPUT_DIR}" ]; then
        echo "âŒ Stage ${stage} output directory not found: ${CURRENT_STAGE_OUTPUT_DIR}"
        exit 1
    fi
    
    # æ›´æ–°ä¸Šä¸€ä¸ª stage çš„è¾“å‡ºç›®å½•
    PREV_STAGE_OUTPUT_DIR="${CURRENT_STAGE_OUTPUT_DIR}"
    
    # è®¡ç®— stage è€—æ—¶
    STAGE_END_TIME=$(date +%s)
    STAGE_DURATION=$((STAGE_END_TIME - STAGE_START_TIME))
    STAGE_HOURS=$((STAGE_DURATION / 3600))
    STAGE_MINS=$(((STAGE_DURATION % 3600) / 60))
    
    echo ""
    echo "â±ï¸  Stage ${stage} took ${STAGE_HOURS}h ${STAGE_MINS}m"
    echo "============================================================================"
done

# è®¡ç®—æ€»è€—æ—¶
END_TIME=$(date +%s)
TOTAL_DURATION=$((END_TIME - START_TIME))
TOTAL_HOURS=$((TOTAL_DURATION / 3600))
TOTAL_MINS=$(((TOTAL_DURATION % 3600) / 60))

# æ„å»ºæœ€ç»ˆ checkpoint è·¯å¾„ï¼ˆä½¿ç”¨æœ€åä¸€ä¸ª stage çš„æœ€ç»ˆè®­ç»ƒæ­¥æ•°ï¼‰
FINAL_STAGE=${STAGES[-1]}
FINAL_CHECKPOINT_STEP=${STAGE_STEPS[$FINAL_STAGE]}
FINAL_CHECKPOINT="${PREV_STAGE_OUTPUT_DIR}/checkpoints/steps_${FINAL_CHECKPOINT_STEP}_pytorch_model.pt"

echo ""
echo "============================================================================"
echo "âœ… All stages completed successfully!"
echo "============================================================================"
echo "Total training time: ${TOTAL_HOURS}h ${TOTAL_MINS}m"
echo "Final checkpoint: ${FINAL_CHECKPOINT}"
echo "============================================================================"
